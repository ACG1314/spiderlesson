# 第四章 静态网页爬取

## 4.1 网页结构

网页可以分为三大部分：HTML CSS和JavaScript

1. **HTML** 定义了网页的内容
2. **CSS** 描述了网页的布局
3. **JavaScript** 控制了网页的行为

### 4.1.1 HTML -- 超文本标记语言

​	HTML是用来描述网页的一种语言， 其全称叫作HyperText Markup Language， 即超文本标记语言。

​	网页包括文字、按钮、图片和视频等各种复杂的元素， 其基础架构就是HTML。不同类型的文字通过不同类型的标签来表示，如图片用img标签表示， 视频用video标签表示， 段落用p标签表示，

​	它们之间的布局又常通过布局标签div嵌套组合而成，各种标签通过不同的排列和嵌套才形成了网页的框架。

```html
<html>

<head>
  这里是文档的头部 ... ...
  ...
</head>

<body>
  这里是文档的主体 ... ...
  ...
</body>

</html>
```

### 4.1.3 CSS

​	HTML定义了网页的结构， 但是只有HTML页面的布局并不美观， 可能只是简单的节点元素的排列， 为了让网页看起来更好看一些， 这里借助了CSS。

​	CSS， 全称叫作Cascading StyleSheets， 即层叠样式表。“层叠”是指当在HTML中引用了数个样式文件，并且样式发生冲突时，浏览器能依据层叠顺序处理。“样式”指网页中文字大小、颜色、元素间距、排列等格式。

​	CSS是目前唯一的网页页面排版样式标准， 有了它的帮助， 页面才会变得更为美观。

```css
body {
    background-color:#d0e4fe;
}
h1 {
    color:orange;
    text-align:center;
}
p {
    font-family:"Times New Roman";
    font-size:20px;
}
```

### 4.1.4 JavaScript

​	JavaScript， 简称JS， 是一种脚本语言。HTML和CSS配合使用， 提供给用户的只是一种静态信息，缺乏交互性。

​	我们在网页里可能会看到一些交互和动画效果，如下载进度条、提示框、轮播图等，这通常就是JavaScript的功劳。它的出现使得用户与信息之间不只是一种浏览与显示的关系， 而是实现了一种实时、动态、交互的页面功能。

​	JavaScript通常也是以单独的文件形式加载的， 后缀为js， 在HTML中通过script标签即可引入。

```javascript
<script>
alert("我的第一个 JavaScript");
</script>
```



## 4.2 urllib

### 前言

​	我们之前已经学习过了，爬虫其实就是模拟浏览器向服务器发出请求，那么我们需要从哪个地方做起，需要我们自己来构造请求报文吗？需要我们自己去传输通信吗？

​	在Python中，已经提供很多个功能强大的HTTP请求库来帮我们完成请求，最基础的HTTP库有**urllib，httplib2，requests，treq**等。

​	其中**urllib**是Python自带的HTTP请求库

​	在Python2中，有urllib和urllib2两个库来实现请求的发送。Python3将两个库统一为urllib库。

它一共有4大模块

- **request**  它是最基本的HTTP请求模块，可以用于模拟发送请求。
- **error**  异常处理模块，如果出现请求错误，可以用来捕获这些通信异常
- **parse**  一共工具模块，提供了许多URL处理方法，比如拆分、解析、合并等
- **robotparser**  主要用于识别网站robots.txt文件，用得比较少(懂的都懂)

### 4.2.1 发送请求

首先导入urllib模块

```python
import urllib.request
```

使用urllib的request模块，可以方便的实现请求的发送并得到响应

#### ①urlopen()

urllib.request.urlopen()

```python
urllib.request.urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
            *, cafile=None, capath=None,cadefault=False,context=None)
```

**[参数解释]**

- **url**:  访问网站的url地址
- **data**:  urllib判断参数是否为None来判断请求方式。默认值为None，为None时表示是GET请求；反之为POST请求

- **timeout**:  超时设置

- **cafile、capath、cadefault** : 组成一组HTTPS请求的可行CA证书，cafile应指向包含一组CA证书的单个文件；capath指向证书文件的目录，cadefault设置默认值

- **context**  描述各种SSL选项的实例



![](F:\学院杂物\主要文件夹--2022年第一学期课程材料\2022_Python网络爬虫材料\网络爬虫实训手册\img文件\橙汁餐厅.jpg)

让我们向橙汁园餐厅（http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index）发送一个请求

```python
import urllib.request

# urllib.requests.urlopen()发送请求
# response用来接收 urlopen()请求收到的响应数据
response = urllib.request.urlopen('http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')

print(response.read().decode('utf-8')) # .read()打印响应主体
```

urlopen()的返回值就是服务器的响应数据，为了方便教学，我们统一约定使用**response**作为响应数据的标识符

#### **②response**

```python
import urllib.request
response = urllib.request.urlopen('http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')
print(type(response)) #  <class 'http.client.HTTPResponse'>
```

response是一个HTTPResponse类型的对象

主要包含

- **response.read()**  获取响应数据

```python
import urllib.request

response = urllib.request.urlopen('http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')
print(type(response)) # <class 'http.client.HTTPResponse'>
print(response.read().decode('utf-8')) # .read()打印响应数据
```



- **response.status / response.getcode()**  状态码
- **response.getheaders()** 获取全部响应头信息
- **response.getheaders('Server')**  获取'Server'响应头信息

```python
import urllib.request

response = urllib.request.urlopen('http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')
print(response.status)  # 获取状态码
print(response.getcode()) # 获取状态码
print(response.getheaders()) # 获取全部头信息
print(response.getheader('Server')) #获取Server头信息
```



- **response.geturl()**  获取实际请求的页面url(防止重定向用)

```python
import urllib.request

response = urllib.request.urlopen('http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')

# 获取实际请求的页面url(防止重定向用)
print(response.geturl())
```



- **response.reason**  获取响应描述字符串

```python
import urllib.request

response = urllib.request.urlopen('https://www.python.org')
print(response.reason) # 获取响应描述字符串
```



- **response.version**  获取协议号

```python
import urllib.request

response = urllib.request.urlopen('https://www.python.org')
print(response.version) # 获取协议号
```



#### ③data参数

​	data参数是**POST**请求专用参数。

​	在urlopen()方法中，data必须是**字节流**类型，即**byte**类型，需要先通过**bytes()**方法转化

```python
import urllib.parse
import urllib.request

data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
response = urllib.request.urlopen('http://httpbin.org/post', data=data)
print(response.read())
```

​	有时候会用作登陆请求上

​	下面是登陆PSNINE  http://www.psnine.com/sign/in

​	ps：这是我的个人账号，求求各位别乱搞我！！！！！！！

```python
import urllib.parse
import urllib.request

data = bytes(urllib.parse.urlencode({'psnid': 'ACG1314','pass':"t8b7"}), encoding='utf8')
response = urllib.request.urlopen('http://www.psnine.com/sign/signin/ajax', data=data)
print(response.read())
```

#### ④timeout参数

​	timeout参数用于设置超时时间，单位为秒

在timeout=1时，成功

```python
import urllib.request

response = urllib.request.urlopen('http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index', timeout=1)
print(response.read())
```

在timeout=0.01时报错

```python
import urllib.request

response = urllib.request.urlopen('http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index', timeout=0.01)
print(response.read())
```

timeout参数配合异常处理可以提高爬虫的效率

### 4.2.2 Request类发送复杂请求

​	urlopen()只有几个简单的参数，是不足以构建一个完整的请求。如果想要在请求中加入**Hearders**等信息，就需要**Request**类来构建。

​	先来看一下样例代码

```python
from urllib import request, parse

url = 'http://httpbin.org/post'
headers = {
    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
    'Host': 'httpbin.org'
}  # 构造头信息
dict = {
    'name': 'Germey'
}
data = bytes(parse.urlencode(dict), encoding='utf8')
req = request.Request(url=url, data=data, headers=headers, method='POST')  #构造请求类
response = request.urlopen(req)
print(response.read().decode('utf-8'))

req = request.Request(url=url, data=data, method='POST')
req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')  #添加一个头信息
```

**urliib.request.Request**的语法如下

```python
urllib.request.Request(url,data=None,headers={},method=None)
```

- url:连接
- data:POST数据
- headers:头信息，接收字典
- method:请求方法

```python
"""
伪装UA浏览器向橙汁餐厅发送请求
"""
from urllib import request

url = "http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index"

# 自定义请求头
headers = {
    "User-Agent":"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3883.400 QQBrowser/10.8.4559.400"
}

# 设置request的请求头
req = request.Request(url = url,data=None,headers=headers,method="GET")

# 发送请求
response = request.urlopen(req)

# 输出响应主体
print(response.read().decode("utf-8"))
```

​	有些网站会做UA封锁，不伪装UA的话，会被封爬虫

**练习：试一下爬取百度搜索（https://www.baidu.com/s?wd=spider-man）的页面，分别尝试一下伪装UA和不伪装UA的情况**

### 4.2.3 异常处理

​	在**urllib库**的**error类**定义了由**request**产生的异常

##### ① URLError

​	报错页面：url打开一个不存在的页面

```python
from urllib import request, error
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.URLError as e:
    print(e.reason)
```

##### ② HTTPError

​	专门用于处理HTTP请求错误，比如认证请求错误

- **code**：返回HTTP状态码，404网页不存在，500服务器炸了
- **reason**：返回错误原因
- **header** ：返回请求头

```python
from urllib import request,error
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
```

### 4.2.4 解析链接

​	刚刚的代码中，还出现了一个叫做**parse**的模块，它是什么作用呢？

​	我们来看一下这个链接

<img src="F:\学院杂物\主要文件夹--2022年第一学期课程材料\2022_Python网络爬虫材料\网络爬虫实训手册\img文件\想看新蝙蝠侠.jpg" style="zoom: 80%;" />

​	但是我们复制一下链接：https://www.baidu.com/s?wd=%E6%96%B0%E8%9D%99%E8%9D%A0%E4%BE%A0

​	像是中文关键词就是进行过编码的，**parse模块**就是用于处理URL的标准接口，例如实现**URL各部分的抽取，合并以及链接转换**。

##### ① urlparse 和 urlunparse

​	该方法实现URL的识别和分段

```python
from urllib.parse import urlparse

result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')
print(type(result), result) # <class 'urllib.parse.ParseResult'> 
```

​	返回结果是一个ParseResult类

​	包含六个属性

```
scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')
```

​	与之相反

```python
from urllib.parse import urlunparse

data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
print(urlunparse(data))
```

##### ② urlsplit 和 urlunsplit

```python
from urllib.parse import urlsplit

result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')
print(result) 
# SplitResult(scheme='http', netloc='www.baidu.com', path='/index.html;user', query='id=5', fragment='comment')
```

```python
from urllib.parse import urlunsplit

data = ['http', 'www.baidu.com', 'index.html', 'a=6', 'comment']
print(urlunsplit(data))  # http://www.baidu.com/index.html?a=6#comment
```

##### ③ urljoin

​	用于多个之间链接的拼接

```python
from urllib.parse import urljoin

print(urljoin('http://www.baidu.com', 'FAQ.html'))
print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))
print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))
print(urljoin('http://www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com#comment', '?category=2'))
```

##### ④ urlencode()

​	它是构造GET请求参数时非常重要的方法，比如：

```python
from urllib.parse import urlencode

params = {
    'name': 'germey',
    'age': 22
}
base_url = 'http://www.baidu.com?'
url = base_url + urlencode(params)
print(url)
```

​	先将GET请求参数字典，再通过**urlencode**加入到url中

##### ⑤ quote()和unquote()

​	主要用于中文参数

```python
from urlllb.parse import quote

keyword = '新蝙蝠侠确实牛逼'
url = 'https://www.baidu.com/s?wd=' + quote(keyword)
print(url)
```

```python
from urllib.parse import unquote

url = 'https://www.baidu.com/s?wd=%E6%96%B0%E8%9D%99%E8%9D%A0%E4%BE%A0%E6%80%8E%E4%B9%88%E8%BF%98%E4%B8%8D%E5%AE%9A%E6%A1%A3'
print(unquote(url))
```

 

## 4.3 requests库

​	在上一节的学习中，我们学习了urllib库的基本用法，但是其中确实有不方便的地方。

​	为了更加方便的去实现这些操作，就有了更加强大且简便的库---requests库。

### 4.3.0 下载requests

​	Windows终端输入 `pip install requests`

![image-20220316112718871](C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316112718871.png)



​	Linux系统终端输入`sudo pip install requests`



### 4.3.1 requests库发送请求

​	`urllib库`中把`GET请求`和`POST请求`封装到一个`urlopen()`方法，`requests库`中将`GET请求`和`POST请求`分成不同的两个方法

​	ps：`requests库`中还有`put()`和`delete()`发送`PUT请求`和`DELETE请求`



​	首先导入`requests模块`

```python
import requests
```



#### **① requests.get()**

​	我们先来看一下`get()`的构造

```python
def get(url, params=None, **kwargs):
    kwargs.setdefault('allow_redirects', True)
    return request('get', url, params=params, **kwargs)    
```



​	我们向订餐系统网站发送一个GET请求

```python
import requests
# ctrl+左键 查看一下request.models
import requests.models

url = 'http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index'

#使用requests发送GET请求
response = requests.get(url = 'http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')

#返回一个Response对象
print(response) # <Response [200]>
print(type(response)) # <class 'requests.models.Response'>
```

​	`get()`方法返回的是一个`Response对象`



#### **② response类**

​	和urllib一样，requests库发起请求后会返回一个Response对象，它是一个HTTP响应类

主要包含以下API：

- **response.content **  获取响应数据(byte类型)

```python
import requests

url = 'http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index'
response = requests.get(url = 'http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')

#响应数据
print(response.content)  # 返回二进制数据
```

​	在urllib库中，我们想获取文本数据，还需要`response.read().decode("utf-8")`进行解码，而在requests库中直接封装了一个属性获取文本数据



- **response.text 获取文本数据**，省去解码过程

```python
import requests

url = 'http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index'
response = requests.get(url = 'http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')

#获取响应正文(文本)
print(response.text)
```

​	如果响应数据类型是JSON格式，requests库也封装了response.json()方法获取JSON格式数据



- **response.json() 获取JSON格式数据**  Requests中内置的JSON解码器

```python
import requests

# ps：这部分的内容我们以后会学到，现在大家只用看最后一行response.json()的数据格式
headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'
}
url = 'https://movie.douban.com/j/chart/top_list'
params = {
    'type': '5',
    'interval_id': '100:90',
    'action': '',
    'start': '10',
    'limit': '50',
}
response = requests.get(url=url,params=params,headers=headers)
#.json()将获取的字符串形式的json数据序列化成字典或者列表对象
page_text = response.json()

print(page_text) # object --> dict
```



- **response.status_code **   获取响应状态码
- **response.headers  **   全部头信息
- **response.ok 请求是否成功**   如果status_code小于200，response.ok返回True；大于200，返回False
- **response.url**  获取实际请求的页面url(防止重定向用) 

```python
import requests

url = 'http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index'
response = requests.get(url = 'http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')

#响应状态码
print(response.status_code)  # 200
#响应头信息
print(response.headers) # 
#请求是否成功
print(response.ok) # True
#实际请求的页面url
print(response.url) # http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index
```



- **response.reason **  获取响应描述字符串

```python
import requests

response = requests.get('https://www.python.org')
# 获取响应描述字符串 
print(response.reason)  # ok
```



- **response.cookies  **  获取cookie信息

![img](https://images2018.cnblogs.com/blog/624465/201803/624465-20180319175811203-882087988.png)

```python
import requests

response = requests.get('http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')
# 获取cookies
print(response.cookies)  
# <RequestsCookieJar[<Cookie JSESSIONID=12CAA6B0EB3BB3C5B916AB5CC6557B47 for 120.79.0.124/lcvc_ebuy_jsp>]>
```



- **response.enconding**  获取响应的编码格式

```python
import requests

response = requests.get('http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')
# 获取响应的编码格式
print(response.encoding)  # UTF-8
# 可以通过修改response.encoding来修改编码格式
response.encoding = "gbk"
print(response.encoding)  # gbk
```

#### 补充:Requests类

**reponse.request **  返回此响应的请求对象`Requests类`

- **response.request.body    **  返回请求对象中的请求体数据
- **response.requests.headers **  返回请求对象中的请求头信息

GET请求得到的`response`对象中的请求对象的请求体`request.body`为`None`,POST请求的`request.body`是`data`

```python
import requests

url = 'http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index'
response = requests.get(url = 'http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')

print(response.request.body)  # 请求正文  GET请求为None

data = {"psnid":"ACG1314","pass":"t8b7"}
url ="http://www.psnine.com/sign/signin/ajax"
response = requests.post(url=url,data=data)

print(response.request) # 请求对象  <PreparedRequest [POST]>
print(response.request.headers) # 请求头信息    {'User-Agent': 'python-requests/2.21.0'............}
print(response.request.body) # 请求体  psnid=ACG1314&pass=t8b7
```



#### ③ requests.post()

`requests.post()`是发送`post请求`，在requests库中，不需要把`data参数`编码成报文格式，直接传入`字典类型`就可以

```python
def post(url, data=None, json=None, **kwargs):
	return request('post', url, data=data, json=json, **kwargs)
```



​	相比urllib库，我们来看一下用requests库发送post请求

​	下面是登陆PSNINE  http://www.psnine.com/sign/in

```python
import requests
url = "http://www.psnine.com/sign/signin/ajax"
data = {"psnid":"ACG1314","pass":"t8b7"} # requests库中不需要把data转换成报文格式
response = requests.post(url=url,data=data)  # 直接传入key-value结构
print(response.text) # success
```



​	下面是去登陆订餐系统

```python
import requests

# 获取第一代订餐系统登录页面，该请求是传统的表单post登录请求
url = "http://120.79.0.124:8080/lcvc_ebuy_jsp/admin/login"
form = {  # 要传递的数据,requests库中不需要把data转换成报文格式
    'username': 'user',
    'password': '123456'
}
# 发送post请求
response = requests.post(url=url, data=form)
print(response.text)  # 输出登录成功或登录失败后的页面
```



​	比如`urllib库`，`requests`发送`post请求`要方便得多，不仅如此，`requests库`发送复杂请求也要比`urllib库`要方便，并且更加强大

#### ④ requests库继承urllib3的错误类(参考用)

```python
class HTTPError(RequestException):
    """An HTTP error occurred."""
    """发生 HTTP 错误。"""
    
class ConnectionError(RequestException):
    """A Connection error occurred."""
    """发生连接错误。"""
    
class ProxyError(ConnectionError):
    """A proxy error occurred."""
    """发生代理错误"""
    
class SSLError(ConnectionError):
    """An SSL error occurred."""
    '''发生SSL验证错误'''
    
class Timeout(RequestException):
    """The request timed out.
    Catching this error will catch both
    :exc:`~requests.exceptions.ConnectTimeout` and
    :exc:`~requests.exceptions.ReadTimeout` errors.
    """
    '''超时错误'''
class ConnectTimeout(ConnectionError, Timeout):
    """The request timed out while trying to connect to the remote server.
    Requests that produced this error are safe to retry.
    """
    """请求在尝试连接到远程服务器时超时。产生此错误的请求可以安全地重试。"""

class ReadTimeout(Timeout):
    """The server did not send any data in the allotted amount of time."""
    """服务器在规定的时间内没有发送任何数据。"""

class URLRequired(RequestException):
    """A valid URL is required to make a request."""
    """发出请求需要有效的 URL。"""

class TooManyRedirects(RequestException):
    """Too many redirects."""
    """重定向太多。"""

class MissingSchema(RequestException, ValueError):
    """The URL schema (e.g. http or https) is missing."""
    """缺少 URL 架构（例如 http 或 https）。"""

class InvalidSchema(RequestException, ValueError):
    """See defaults.py for valid schemas."""
    """查看 defaults.py 以获得有效的模式。"""

class InvalidURL(RequestException, ValueError):
    """The URL provided was somehow invalid."""
    """提供的 URL 不知何故无效。"""

class InvalidHeader(RequestException, ValueError):
    """The header value provided was somehow invalid."""
        """提供的头信息值不知何故无效。"""

class InvalidProxyURL(InvalidURL):
    """The proxy URL provided is invalid."""
    """提供的代理 URL 无效。"""

class ChunkedEncodingError(RequestException):
    """The server declared chunked encoding but sent an invalid chunk."""
    """服务器声明了分块编码，但发送了一个无效的块。"""

class ContentDecodingError(RequestException, BaseHTTPError):
    """Failed to decode response content"""、
    """解码响应内容失败"""

class StreamConsumedError(RequestException, TypeError):
    """The content for this response was already consumed"""
    """此响应的内容已被使用"""
    
class RetryError(RequestException):
    """Custom retries logic failed"""
    """自定义重试逻辑失败"""

class UnrewindableBodyError(RequestException):
    """Requests encountered an error when trying to rewind a body"""
    """请求在尝试回退正文时遇到错误"""

# Warnings
class RequestsWarning(Warning):
    """Base warning for Requests."""
    """请求的基本警告。"""
    pass

class FileModeWarning(RequestsWarning, DeprecationWarning):
    """A file was opened in text mode, but Requests determined its binary length."""
    """一个文件以文本模式打开，但 Requests 确定了它的二进制长度。"""
    pass

class RequestsDependencyWarning(RequestsWarning):
    """An imported dependency doesn't match the expected version range."""
    """导入的依赖项与预期的版本范围不匹配。"""
    pass
```

### 4.3.2 requests库发送复杂请求

用`ctrl+左键`点击一下`requests.get()`

```python
# ctrl+左键 对着 get()
response = requests.get()
```

跳转到`get()`的底层代码`requests.api`



我们首先来看一下`requests.api`中的底层代码

```python
def request(method, url, **kwargs):
    """Constructs and sends a :class:`Request <Request>`.

    :param method: method for the new :class:`Request` object.
    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the body of the :class:`Request`.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
    :param headers: (optional) Dictionary of HTTP Headers to send with the :class:`Request`.
    :param cookies: (optional) Dict or CookieJar object to send with the :class:`Request`.
    :param files: (optional) Dictionary of ``'name': file-like-objects`` (or ``{'name': file-tuple}``) for multipart encoding upload.
        ``file-tuple`` can be a 2-tuple ``('filename', fileobj)``, 3-tuple ``('filename', fileobj, 'content_type')``
        or a 4-tuple ``('filename', fileobj, 'content_type', custom_headers)``, where ``'content-type'`` is a string
        defining the content type of the given file and ``custom_headers`` a dict-like object containing additional headers
        to add for the file.
    :param auth: (optional) Auth tuple to enable Basic/Digest/Custom HTTP Auth.
    :param timeout: (optional) How many seconds to wait for the server to send data
        before giving up, as a float, or a :ref:`(connect timeout, read
        timeout) <timeouts>` tuple.
    :type timeout: float or tuple
    :param allow_redirects: (optional) Boolean. Enable/disable GET/OPTIONS/POST/PUT/PATCH/DELETE/HEAD redirection. Defaults to ``True``.
    :type allow_redirects: bool
    :param proxies: (optional) Dictionary mapping protocol to the URL of the proxy.
    :param verify: (optional) Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. Defaults to ``True``.
    :param stream: (optional) if ``False``, the response content will be immediately downloaded.
    :param cert: (optional) if String, path to ssl client cert file (.pem). If Tuple, ('cert', 'key') pair.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response

    Usage::

      >>> import requests
      >>> req = requests.request('GET', 'https://httpbin.org/get')
      <Response [200]>
    """

    # By using the 'with' statement we are sure the session is closed, thus we
    # avoid leaving sockets open which can trigger a ResourceWarning in some
    # cases, and look like a memory leak in others.
    with sessions.Session() as session:
        return session.request(method=method, url=url, **kwargs)
    
    
    
def get(url, params=None, **kwargs):
    r"""Sends a GET request.

    :param url: URL for the new :class:`Request` object.
    :param params: (optional) Dictionary, list of tuples or bytes to send
        in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """

    kwargs.setdefault('allow_redirects', True)
    return request('get', url, params=params, **kwargs)




def post(url, data=None, json=None, **kwargs):
    r"""Sends a POST request.

    :param url: URL for the new :class:`Request` object.
    :param data: (optional) Dictionary, list of tuples, bytes, or file-like
        object to send in the body of the :class:`Request`.
    :param json: (optional) json data to send in the body of the :class:`Request`.
    :param \*\*kwargs: Optional arguments that ``request`` takes.
    :return: :class:`Response <Response>` object
    :rtype: requests.Response
    """

    return request('post', url, data=data, json=json, **kwargs)
```

​	首先它定义一个HTTP请求方法`request()` ,里面定义了诸多参数，`get()`,`post()`方法实际上都是调用了`request()`方法，所有我们在传参的时候可以去参考`request()`方法的参数介绍，我们以下就以几个比较常见的参数做介绍 。

​	ps：`requests.request()`其实是调用了`requests.session()`发送请求，感兴趣的同学可以自己去了解一下源码



#### ① headers参数  

​	`headers参数`用于伪造头信息，接收`字典(key-value)类型`

​	以百度为例，当我们不伪装`User-Agent`去爬取百度搜索时，百度会返回给我们一个错误页面

```python
import requests

url = "https://www.baidu.com/s?wd=Python"
response = requests.get(url=url)
fp = open("./t1.html",'wt',encoding='utf-8')  # 写入磁盘
fp.write(response.text)
fp.close()
print(response.request.headers) # {'User-Agent': 'python-requests/2.21.0', ...........}
```

​		结果是一个错误页面,这是因为我们用`python.requests`写爬虫的时候，`请求头`信息中的UA是`'python-requests/2.21.0'`,触发了百度的反爬策略



<img src="C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316180340558.png" alt="image-20220316180340558" style="zoom: 33%;" />





​	当我们传入携带了伪装User-Agent的请求头参数时，那么就能做到反反爬了

```python
import requests

#伪装请求头
headers = {"User-Agent":
               "MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GRJ22; CyanogenMod-7) "
               "AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1",
           "Host":"www.baidu.com"}

url = "https://www.baidu.com/s?wd=Python"
response = requests.get(url=url,headers=headers)  # headers是请求头参数，接收字典类型

fp = open("./t1.html",'wt',encoding='utf-8')  # 写入磁盘
fp.write(response.text)
fp.close()

print(response.request.headers) # {'User-Agent': 'MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2...............
```

<img src="C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316181018013.png" alt="image-20220316181018013" style="zoom: 25%;" />





​	由于我们在`请求头`信息中伪装的UA是一个移动端UA，所以看到页面是一个移动端页面

#### ② timeout参数

```python
import requests

url = "http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index"
response = requests.get(url=url,timeout=1)  # 超时参数
# response = requests.get(url=url,timeout=0.001)  # 时间太短，报错
print(response) # <Response [200]>
```



#### ③ params参数

`params`参数是一个查询参数，用于接收`GET请求`中的查询参数，接收类型为`字典类型`。

​	我们知道`GET请求`的查询是放在`url`中，比如https://www.baidu.com/s?wd=Python&pn=10 中的 `wd`和`pn`就是查询参数

​	如果查询参数太多，那么`url`就会非常长。

​	在`requests库`中，可以用`params`参数接收查询参数，url就只用写https://www.baidu.com/s 了

```python
import requests

#简写url
url = "https://www.baidu.com/s"

#伪装请求头
headers = {"User-Agent":
               "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/5"
               "37.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36",
           "Host":"www.baidu.com"}

#GET请求查询参数--字典形式
params = {"wd":"Python","pn":10}

#携带伪装头和查询参数发送GET请求
response = requests.get(url=url,headers=headers,params=params)

print(response)


#做一个持久化存储
fp = open("./t2.html","wt",encoding="utf-8")
fp.write(response.text)
fp.close()
```

#### 实训一：GET请求登陆订餐系统（params参数形式）

​	第二代订餐系统网址：

​	登录请求为get方式，通过下面方式传值：
http://120.79.0.124:8081/api/shop/customer/login?username=user&password=123456

```python
# 第二代订餐系统客户登录
# 登录请求为get方式，通过下面方式传值：
# http://120.79.0.124:8081/api/shop/customer/login?username=user&password=123456
url = "http://120.79.0.124:8081/api/shop/customer/login"
params = {  # 要传递的数据,该接口参数以query方式传递
    'username': 'wenye',
    'password': '1234561'
}
response = requests.get(url=url, params=params)  
# params的时候之间接把参数加到url后面
# 只在get请求时使用：params 参数是一个查询参数，用于接收 GET请求 中的查询参数，接收类型为 字典类型 。
data = response.json()  # {'code': 0}
if data.get("code") == 0:
    print("登录成功")
else:
    print(data.get("msg"))  # 登录失败：密码错误，您还有98次机会
```



#### ④ proxies参数

设置代理

1. 定义: 代替你原来的IP地址去对接网络的IP地址。

2. 作用: 隐藏自身真实IP,避免被封。

接收以下格式

```python
# 1、语法结构
       proxies = {
           '协议':'协议://IP:端口号'
       }
# 2、示例
    proxies = {
        'http':'http://IP:端口号',
        'https':'https://IP:端口号'
    }
```

​	我们可以去一些免费或付费代理的网站（比如说西刺代理、快代理、全网代理、代理精灵、... ... ），也可以自己搭服务器

```python
import requests


url = 'http://httpbin.org/get'

# 定义代理,在代理IP网站中查找免费代理IP
proxies = {
    'http':'122.9.101.6:8888',
}
response = requests.get(url,proxies=proxies,timeout=10)
print(response.text)
```

ps：上面的IP不一定能成功，免费的代理IP要经常维护，练习的时候自己去找可用IP。

<img src="C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316202508041.png" alt="image-20220316202508041" style="zoom: 50%;" />



#### ⑤ verify参数

```
1、适用网站: https类型网站但是没有经过 证书认证机构 认证的网站
2、适用场景: 抛出 SSLError 异常则考虑使用此参数
```

​	正常的向12306发起请求

```python
import requests

# 向12306发起请求
response = requests.get("https://12306.cn") # 报错 requests.exceptions.SSLError:
```

​	这是一个`SSLEroor`，表示证书验证错误，我们可以通过设置`verify`参数为`False`就行

```python
import requests

# 设置SSL验证为False,向12306发起请求
response = requests.get("https://12306.cn",verify=False)
print(response) # <Response [200]>
```

​	可以看到，还是报了一些警告（无伤大雅），其实也没什么关系，如果看着不舒服，我们还可以设置忽略警告

```python
import requests
from requests.packages import urllib3

# 忽略警告
urllib3.disable_warnings()
# 设置SSL验证为False,向12306发起请求
response = requests.get("https://12306.cn",verify=False)
print(response) # <Response [200]>
```



#### 实训二：通过爬虫实现对第二代订餐系统前台模拟客户注册

​	注册地址：http://120.79.0.124/ebuy_web_mobile/#/Reg

​	这里是通过POST请求注册用户：如下





<img src="C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220322095502856.png" alt="image-20220322095502856" style="zoom: 33%;" />





​	可以具体看一下它的头信息和参数





<img src="C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220322095745683.png" alt="image-20220322095745683" style="zoom:50%;" />



<img src="C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220322095819657.png" alt="image-20220322095819657" style="zoom: 50%;" />



```python
import requests

# 第二代订餐系统前台客户注册模拟
# 注意体会有邀请码和没有邀请码的注册方式
url = "http://120.79.0.124:8081/api/shop/customer/reg"
form = {
    "address": "",  # 可以绕过注册系统的地址验证，因为网站只在js进行地址费控验证，服务端程序没有相关验证。
    "name": "测试员2号",
    "username":"ljy6",
    "password": "123456",
    "sex": "1",
    "tel": "17376054871",  #无法绕过，因为服务端程序有验证。
    "zip": "000000"
}
invite = {  #邀请码，只有在后台开通后才需要
    "inviteCode": "dashuju"
}
response = requests.post(url=url, params=invite, json=form)
html = response.json()
print(html)
```







#### 实训三：简易网页采集器 

- 实现一个简易网页采集器
  - 基于搜狗(https://www.sogou.com/web)针对指定不同的关键字将其对应的页面数据进行爬取
  - 参数动态化：
    - 如果请求的url携带参数，且我们想要将携带的参数进行动态化操作那么我们必须：
      - 1.将携带的动态参数以键值对的形式封装到一个字典中
      - 2.将该字典作用到get方法的params参数中即可
      - 3.需要将原始携带参数的url中将携带的参数删除

```python
#页面采集器
import requests



keyword = input('输入关键词：\n')
page_count = int(input('选择第几页：\n'))
#浏览器UA伪装  -- 伪装成浏览器  对请求头信息进行修改
#请求报文携带的请求头信息

headers={'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'
             }

#get请求的传参
params = {
    'query':keyword,
    'page':page_count
}

#url = 'https://www.baidu.com/s?wd={}'.format('EDG')
url = 'https://www.sogou.com/web'


response = requests.get(url=url,headers=headers,params=params,timeout = 2)    #发起请求
'''
requests.get()的参数说明
    1.url
    2.headers: 请求头信息（字典）
    3.parmas:get请求的参数(字典)
    4.timeout : 设置超时时间
'''

response.encoding = 'utf-8'     #更改响应报文的编码

page = response.text    #响应主体的超文本

#持久化存储
with open('{}第{}页页面采集.html'.format(keyword,page_count),'wt',encoding='utf-8') as fp:
    fp.write(page)
```



### 4.3.3 请求并保存图片(视频，音频)

#### ① 图片

​	网上的链接不仅仅是html格式，还有比如图片，视频，音频等格式，我们向这些链接发起请求，同样的，我们就会获得这些响应数据

​	以这个图片为例http://120.79.0.124:8080/lcvc_ebuy_jsp/upload/image/20170907/20170907232230_82.jpg





![img](http://120.79.0.124:8080/lcvc_ebuy_jsp/upload/image/20170907/20170907232230_82.jpg)

​	

​	我们向这个图片链接发起请求（以下是错误方式）

```python
import requests

#向蜜雪冰城发起请求
response = requests.get('http://120.79.0.124:8080/lcvc_ebuy_jsp/upload/image/20170907/20170907232230_82.jpg')

print(response) # <Response [200]>

#获取响应文本
print(response.text)
```

![image-20220316205129452](C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316205129452.png)





​	可以看到是乱码，所有我们在接收图片或视频数据，我们不能以`.text`格式接收，要以`.content`格式接收二进制数据

​	以下是正确读取并保存的代码

```python
import requests

# 向蜜雪冰城发起请求
response = requests.get('http://120.79.0.124:8080/lcvc_ebuy_jsp/upload/image/20170907/20170907232230_82.jpg')

print(response) # <Response [200]>

# 获取响应文本
print(response.content)

# 持久化存储
fp = open("./j1.jpg","wb")  # ①这里使用wb模式   ② .jpg .png .jepg等等后缀名都可以
fp.write(response.content)
fp.close()
```



​	之后我们学了更多之后，可以一次性保存很多图片，像我们之后会有个实训，是爬取英雄联盟所有英雄皮肤图片（如下）

![image-20220316205653337](C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316205653337.png)

![image-20220316205731132](C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316205731132.png)



#### ② 视频

​	视频也是一样的（但是速度会很慢，之后我们学习了分布式爬虫后，一定程度上能加速）

```python
import requests

# 向视频发起请求
response = requests.get('https://vd2.bdstatic.com/mda-khu6wi4fi79xiez7/hd/mda-khu6wi4fi79xiez7.mp4?v_from_s=hkapp-haokan-hnb&auth_key=1647437330-0-0-a17d68770cc6e0a14ec8e5acf25bb281&bcevod_channel=searchbox_feed&pd=1&cd=0&pt=3&logid=3530143237&vid=4588112793617850200&abtest=100815_1-17451_1&klogid=3530143237')

print(response) # <Response [200]>


# 持久化存储
fp = open("./w1.mp4","wb")  # ①这里使用wb模式   ② AVI、mov、rmvb、rm、FLV、mp4、3GP等等等后缀名都可以
fp.write(response.content)
fp.close()
```



### 4.3.4 分页爬取

#### ① 初探分页规则

​	每个网站都有自己的分页规则，像我们前面的搜狗搜索，就是很简单的`page=1,2,3,4`，但是同类型的百度就没有那么简单了

​	当我们进行全站爬取时，第一步就是一定要弄清它的分页规则。

​	怎么去探索分页规则呢？

​	很简单，去**【看】**就行了

​	以百度为例，我们一起去**【看】**一下它的分页规则



**第一步：**打开你要爬取的网站主页

​	打开百度搜索，并打开【开发者工具】，找到主页面的数据包，并找到**【Query String Parameters】** 或者**【Form Data】**

<img src="C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316211020954.png" alt="image-20220316211020954" style="zoom: 33%;" />

ps：每个版本浏览器的【开发者工具】的位置可能不同，上面的最新版Chrome就在【Payload】里面（如上图）。



​	如果是旧版，就在【Headers】的最下面（如下图）

<img src="C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316210825023.png" alt="image-20220316210825023" style="zoom: 50%;" />



**第二步：跳转到【第二页】，观察一下参数的变化**

![image-20220316211245240](C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316211245240.png)

​	我们发现发生了一些改变，这时候我们就可以观察哪个参数是控制分页的，那如果这时候还不是很明显，我们继续跳转到【第三页】



**第三步（直到找到分页规则为止）：跳转到下一页，观察一下参数的变化**

<img src="C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316211457068.png" alt="image-20220316211457068" style="zoom:25%;" />

到这里，我发现了一个比较有规律的变化规则：

【第二页】：`pn = 10`

【第三页】：`pn = 20`

以此推出第N页：`pn = 10*(页码 -1)`



**第四步：验证**

​	把链接上多余的参数全部删去，只留下关键的参数

如：

<img src="C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316211814461.png" alt="image-20220316211814461" style="zoom:33%;" />

​	按照现在的规则，我们就知道  
$$
pn = 60 = 10*(页码-1)
=>页码 = 60/10 + 1 = 7
$$
​	如果确实翻到【第七页】就说明分页规则找到了

<img src="C:\Users\ACG1314\AppData\Roaming\Typora\typora-user-images\image-20220316212045409.png" alt="image-20220316212045409" style="zoom: 33%;" />



​	确实是【第七页】，大成功！！！！！！！！！！！！！



​	那我们接下来就可以制作爬取任意页的百度爬虫了（如下）	

```python
import requests

#Baidu
url = "https://www.baidu.com/s"

#伪装头信息
headers = {"User-Agent": "MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GR"
                         "J22; CyanogenMod-7) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1"}

#get请求的传参
params = {'wd':"网络爬虫",'pn':80}  # 我想爬第9页，那就计算 pn = 10*(9-1) = 80,大成功！！！！！

#发起请求
response = requests.get(url=url,params=params,headers=headers)
response.encoding = "utf-8"

# 持久化存储
fp = open("./baidu.html","wt",encoding="utf-8")
fp.write(response.text)
fp.close()
```

​	题外话：我在写实训手册的过程中，发现百度搜索的反爬机制，它不是通过封IP（测试多次），它是通过封UA，如果你用一个假的User-Agent访问多次，那么你这个UA就被封，跳转到【百度安全检测】，因此如果要爬百度的话，要构建【UA池】



#### ② 爬取多页

​	当我们知道了一个网站的分页规则后，我们就可以通过【循环】进行多页爬取，我们以百度为例，爬取1-10页，那么就可以构造一个循环



​	我们只需要把下面这个代码放进循环体内就可以了

```pyhton
    #get请求的传参
    params = {'wd':"网络爬虫",'pn':10*(循环计数-1)}  
    #发起请求
    response = requests.get(url=url,params=params,headers=headers)
    response.encoding = "utf-8"

    # 持久化存储
    fp = open("./baidu{}.html".format(循环计数),"wt",encoding="utf-8")
    fp.write(response.text)
    fp.close()

```



​	完整代码：

```python
import requests

#Baidu
url = "https://www.baidu.com/s"
#伪装头信息
headers = {"User-Agent": "MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GR"
                         "J22; CyanogenMod-7) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1"}

'''
多页爬取，以1-10页为例，那我们就可以构造一个循环
复习一下，在Python中的range()方法是一个生成左闭右开的数组，如果要爬1-10页，那么就要取range(1,11)
'''

for i in range(1,11):
    #get请求的传参
    params = {'wd':"网络爬虫",'pn':10*(i-1)}  # 我想爬第9页，那就计算 pn = 10*(9-1) = 80,大成功！！！！！
    #发起请求
    response = requests.get(url=url,params=params,headers=headers)
    response.encoding = "utf-8"

    # 持久化存储
    fp = open("./baidu{}.html".format(i),"wt",encoding="utf-8")
    fp.write(response.text)
    fp.close()
    
    # 提示
    print("已经采集第{}页".format(i))
```

ps：为了方便自己知道爬虫已经采集到哪了，可以向我上面一样给一个【小提示】



#### 实训四：京东分页爬虫

- 实现京东分页（'https://search.jd.com/Search'）爬取
- 要求一：找出分页规则，爬取1-10页的数据
- 要求二(选做): 使用`time`模块，每次爬取一页，休眠5秒
- 要求三：参数动态化：
  - 如果请求的url携带参数，且我们想要将携带的参数进行动态化操作那么我们必须：
    - 1.将携带的动态参数以键值对的形式封装到一个字典中
    - 2.将该字典作用到get方法的params参数中即可
    - 3.需要将原始携带参数的url中将携带的参数删除



完整代码

```python
#1.导入requests库
#2.分析网页
#3.发起请求

import requests
import time

#伪装头信息
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.193 Safari/537.36'
}

params ={
    'keyword':'笔记本电脑',     #搜索词
    'page': None
}

#网站地址
url = 'https://search.jd.com/Search'

#分页爬取京东商品
for n in range(1,11):

    #构造分页规则
    page_count = 2*n - 1
    params['page'] = page_count
    #print(parmas)

    #发起请求
    response = requests.get(url=url,headers=headers,params=params)

    #获取主体数据
    page = response.text


    #持久化存储
    print('正在保存第{}页数据'.format(n))
    fp = open('./京东网页分页/京东商品第{}页.html'.format(n),'wt',encoding=response.encoding)
    fp.write(page)
    fp.close()

    time.sleep(5)   #休眠5秒
```