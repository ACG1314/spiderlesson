# 第十五章 Scrapy框架爬虫

​	在前面的学习中，我们已经学习了很多爬虫的全过程，将不同的功能定义成不同的方法，甚至抽象出模块的概念。

​	在第11章【代理池的构建和使用】中，我们已经掌握了爬虫框架的雏形，实现调度器、队列、请求对象、异常重试机制、异步等，如果我们将各个组件独立处理，把它们定义成不同的模块，慢慢可以形成一个框架。

​	有了框架之后，我们就不必关心爬虫的流程了，异常处理，任务调度等都会集成在框架中。我们只需要关心爬虫的核心逻辑即可，如果页面信息的提取，下一步请求的生成等。这样，不仅可以大大提高开发效率，还能增强爬虫的健壮性。

​	在本章中，我们将要学习一个非常强大并且成熟，稳定的爬虫框架---Scrapy。

## 15.1 异步爬虫（了解向）

​	在学习Scrapy框架，我们先补充一下之前提过但没有系统学习的异步爬虫。

​	我们知道爬虫是IO密集型任务，例如使用requests库来爬取某个站点时，当发出一个请求后，程序必须等待网站返回响应，才能接着运行，而在等待响应的过程中，整个爬虫是一直等待的。

​	比如我们可以通过Flask-Web构造一个这样的IO堵塞的情况：

```
from flask import Flask, render_template
import time


# 实例化一个app
app = Flask(__name__)

# 创建视图函数&路由路径
@app.route('/do')
def index_1():
    # 故意堵塞5秒
    time.sleep(5)
    return render_template('test.html')

@app.route('/la')
def index_2():
    # 故意堵塞5秒
    time.sleep(5)
    return render_template('test.html')

@app.route('/mi')
def index_3():
    # 故意堵塞5秒
    time.sleep(5)
    return render_template('test.html')


if __name__ == '__main__':
    # debug=True表示开启调试模式：服务器端代码被修改后会按下保存键自动重启服务
    app.run(host='127.0.0.1', port=5000, debug=True)
```

​	接着我们通过requests库对这三个路由发起请求，并计算时间

```python
import requests
import logging
import time


# 协程函数
def get_request(url):
    # 进程开始计时
    process_start_time = time.time()
    response = requests.get(url)
    text = response.text
    # 进程结束时间
    process_stop_time = time.time()
    logger.info(f'结束爬虫{url},耗时{process_stop_time - process_start_time}秒')
    return text

# 回调函数
def callback(text):
    print(len(text))

if __name__ == "__main__":
    # 创建日志
    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(name)s %(message)s')
    # 实例化
    logger = logging.getLogger()

    # 要爬取的链接
    urls = [
        'http://127.0.0.1:5000/do',
        'http://127.0.0.1:5000/la',
        'http://127.0.0.1:5000/mi'
    ]
    # 程序开始计时
    program_start_time = time.time()
    # 开始记录
    logger.info(f'程序开始')
    # 开始爬取
    for url in urls:
        logger.info(f'开始爬取{url}')
        get_request(url)


    # 结束全部
    program_stop_time = time.time()
    logger.info(f'程序结束，总耗时{program_stop_time-program_start_time}')
```

​	![image-20220601112847444](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220601112847444.png?lastModify=1698732446)

​	由于我们故意设置了堵塞5秒的情况，访问这个链接需要先等待五秒才能得到结果。虽然说我们平时浏览网页的时候，绝大部分网页的速度还是很快的，如果写爬虫来爬取，那么从发出请求到接收响应的时间不会很长，因此需要我们等待的时间并不长。但是有些网站的加载速度确实比较慢（比如说一些私人站点，境外网站等等），稍慢的可能要1~3秒，更慢的说不定要10秒以上。

​	如果我们就用requests单线程的去爬取数据，总耗时将会非常巨大。

​	这是一个非常典型的串行堵塞。

​	对于这种情况，我们就要使用异步来优化我们的爬虫方案。

### 15.1.1 异步的基本原理

​	要实现异步机制的爬虫，那必须得提到协程。

​	了解协程需要先了解一些基本概念，如果`阻塞和非阻塞`、`同步和异步`、`多进程和多线程和协程`

```
进程和线程，都是一种CPU的执行单元。

进程：表示一个程序的上下文执行活动（打开、执行、保存...）

线程：进程执行程序时候的最小调度单位（执行a，执行b...)

一个程序至少有一个进程，一个进程至少有一个线程。

对操作系统来说，线程是最小的执行单元，进程是最小的资源管理单元。
```

#### ① 阻塞

​	阻塞状态是指程序在未得到所需计算资源时被挂起的状态。程序在等待某个操作完成期间，自身无法继续干别的事情，则称该程序在该操作上是阻塞的。

​	简单来说，就是**A调用B ，A一直等着B的返回，别的事情什么也不干。**

​	常见的阻塞方式有：

- 网络I/O阻塞
- 硬盘I/O阻塞
- 用户输入阻塞
- ....

​	阻塞是无处不在的，包括在CPU切换上下文时，所有进程都无法干事情，它们也会被阻塞。在多核CPU的情况下，正在执行上下文切换操作的核不可被利用。

#### ② 非阻塞

​	程序在等待某操作的过程中，自身不被阻塞，可以继续干别的事情，则称该程序在该操作上是非阻塞的。

​	简单来说：**A调用B，A不用一直等着B的返回，先去忙别的事情了。**



​	ps: 非阻塞并不是任何程序级别、任何情况下都存在的。仅当程序封装的级别可以囊括独立的子程序单元时，程序才可能存在非阻塞状态。



#### ③ 同步

​	**两个（多个）线程步调要一致，要相互协商**，这些线程是同步执行的。

​	两个线程的运行进度各不相同，怎么才能步调一致呢？我们直观的理解就是，快的等慢的呗！**快的阻塞一下等到慢的步调一致即可**。

​	同步的思想是：**所有的操作都做完，才返回给用户**。

​	同步是**有序**的

#### ④ 异步

​	在完成任务的过程中，**两个（多个）线程不需要步调一致，也能完成任务**，此时不相关的线程是异步的。

​	由于步调可以不一致，**快的线程就不需要等慢的线程了**。

​	异步的思想是：**不用等所有操作都做完，就相应用户请求**。

​	异步是**无序**的



#### ⑤ 多进程/多线程

​	表示可以同时执行多个任务，进程和线程的调度是由**操作系统**自动完成。

![image-20220601120851715](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220601120851715.png?lastModify=1698732446)

多进程：

- ​	密集CPU任务，需要充分使用多核CPU资源（服务器，大量的并行计算）的时候，用多进程。
- ​	缺陷：多个进程之间通信成本高，切换开销大。
- ​	Python可用模块： `multiprocessing`



多线程：

- ​	密集I/O任务（网络I/O，磁盘I/O，数据库I/O）使用多线程合适。
- ​	缺陷：同一个时间切片只能运行一个线程，不能做到高并行，但是可以做到高并发。
- Python可用模块：`threading.Thread`、`multiprocessing.dummy`



​	优缺点对比：

![image-20220601120220778](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220601120220778.png?lastModify=1698732446)



补充知识：

![image-20220602153151483](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220602153151483.png?lastModify=1698732446)



#### ⑥ 协程

​	协程，英文叫做coroutine，又称微线程，纤尘，是一种运行在用户太的轻量级线程。正如一个进程可以拥有多个线程一样，一个线程也可以拥有多个协程。

​	最重要的是，协程不是被操作系统内核所管理，而完全是由**程序**所控制（也就是在用户态执行）。

![image-20220601120948442](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220601120948442.png?lastModify=1698732446)

- ​	协程在单线程上执行多个任务，用**函数切换**，开销极小。不通过操作系统调度，没有进程、线程的切换开销。
- ​	多线程请求返回是无序的，哪个线程有数据返回就处理那个线程；而协程返回的数据是有序的。
- ​	缺陷：单线程执行，处理密集CPU和本地磁盘IO的时候，性能较低。处理网络I/O性能还是比较高.





​	我们爬虫所面对的就是网络I/O，所以我们要通过协程的方式实现异步的高性能爬虫。





### 15.1.2 多线程的使用

```python
#!/usr/bin/env python 
# -*- coding:utf-8 -*-
import requests
import time

from multiprocessing.dummy import Pool

urls = [
        'http://127.0.0.1:5000/do',
        'http://127.0.0.1:5000/la',
        'http://127.0.0.1:5000/mi'
    ]
def get_request(url):
    page_text = requests.get(url=url).text
    return len(page_text)

# # 同步代码
# if __name__ == "__main__":
#     start = time.time()
#
#     for url in urls:
#         res = get_request(url)
#         print(res)
#     print('总耗时：',time.time()-start)
# # 异步代码
if __name__ == "__main__":
    start = time.time()
    # 3表示开启线程的数量
    pool = Pool(3) 
    # 使用get_request作为回调函数，需要基于异步的形式对urls列表中的每一个列表元素进行操作
    # 保证回调函数必须要有一个参数和返回值
    result_list = pool.map(get_request,urls)
    print(result_list)
    print('总耗时：', time.time() - start)
```







### 15.1.3 协程的使用

​	从Python3.4开始，就加入了协程的概念。

​	Python3.5后增加了`async模块`，`await模块`，使得协程的实现更为方便。

​	目前Python中使用协程最常用的模块是`asyncio模块`，本节也会以它为基础来介绍协程的用法。

​	全局的流程图大致如下:

![image-20220602150327760](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220602150327760.png?lastModify=1698732446)

#### ① 协程的重要属性

- event_loop：事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，当满足发生条件的时候，就调用对应的处理方法。
- coroutine：中文翻译是协程，在Python中常指代协程对象类型，我们可以将协程对象注册到事件循环中，它会被事件循环调用。我们可以用`async`关键字定义协程方法，这个方法调用时不会立即被执行，而是会返回一个协程对象。
- task：任务，这是对协程对象的进一步封装，包含协程对象的各个状态。
- future：代表将来执行或者没有执行的任务的结果，实际上和task差不多。
- async/await 关键字：python3.5用于定义协程的关键字，async定义一个协程，await用于挂起阻塞的异步调用接口。

#### ② 定义协程并创建任务

​	首先，我们需要导入`asyncio模块`，这样就可以使用`async`和`await`关键字。然后就可以通过`async`定义一个协程方法。

```python
import asyncio

# async定义一个协程方法（异步函数）
# 协程不能直接运行，需要将协程加入到事件循环中
async def get_request(url):
    page_text = requests.get(url=url).text
    # 简单输出一下文本长度
    print(len(page_text))

# 协程不能直接运行，需要将协程加入到事件循环中
coro = get_request('http://127.0.0.1:5000/do')		# 此时未运行
print(coro)	# <coroutine object do_some_work at 0x000002235782D6C8>
```

​	我们用`async`定义一个`get_request()`方法，该方法接收一个数字参数x，执行之后打印这个数字。

​	接下来我们直接调用`get_request()`方法，然而这个方法并没有执行，而是返回了一个coroutine协程对象。



```python
# asyncio.get_event_loop：创建一个事件循环
loop = asyncio.get_event_loop()

# # 创建任务, 不立即执行
# 通过loop.create_task(coroutine)创建task
task = loop.create_task(coro)
# # 同样的可以通过 asyncio.ensure_future(coroutine)创建task
# task = asyncio.ensure_future(coro)

# 使用run_until_complete将协程注册到事件循环，并启动事件循环
loop.run_until_complete(task)
```

​	我们使用`asyncio.get_event_loop()`创建了一个事件循环loop，并通过loop对象根据协程创建一个任务`task`，并调用

`loop.run_until_complete(task)`将任务注册到了事件循环中，接着启动。

​	最终，我们才看到了方法执行的结果。



![image-20220602145936063](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220602145936063.png?lastModify=1698732446)



#### ③ 绑定回调（重点理解）

​	绑定回调，在task执行完成的时候可以获取执行的结果，回调的最后一个参数是future对象，通过该对象可以获取协程返回值。

​	下面代码中，我们将`get_request()`的返回值以**task对象**的形式传递给了回调函数`task_callback()`中。

```python
import asyncio
import requests

# 协程方法
async def get_request(url):
    response = requests.get(url)
    # 返回Response，等下直接交给回调函数 task_callback()
    return response

#  回调函数的封装
#  参数t：就是该回调函数的调用者（任务对象）
#  参数名其实最好直接用response，这里是为了方便对比教学改用t
def task_callback(t):
    print(f'我是回调函数,参数t为{t}, 参数t类型为{type(t)}')
    # result返回的就是特殊函数的返回值
    print(f't.result()返回的是：{t.result()}')
    # 打印响应长度
    print(len(t.result().text))


if __name__ == '__main__':
    # 协程对象
    coro1 = get_request('http://127.0.0.1:5000/do')
    # 创建事件循环
    loop = asyncio.get_event_loop()
    # 创建任务,对协程对象的进一步封装
    task = asyncio.ensure_future(coro1)
    # 给task绑定一个回调函数
    task.add_done_callback(task_callback)
    # 使用run_until_complete将协程注册到事件循环，并启动事件循环
    loop.run_until_complete(task)
```

![image-20220602151823625](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220602151823625.png?lastModify=1698732446)

​	

​		我们通过`task.add_done_callback()`实现了回调方法。但是实际上，即使不使用回调方法，在task运行完毕后，也可以

直接通过`task.result()方法`获取结果。

```python
print(task.result())
```

​	



#### ④ 多任务操作

​	在上面的例子中，我们执行了一次请求，如果想执行多次请求，可以定义一个task列表，然后用`asyncio模块`中的`wait`方法执行。

​	`await`的功能：协程遇到await，事件循环将会挂起该协程，执行别的协程，直到其他的协程也挂起或者执行完毕，再进行下一个协程的执行。

```python
import asyncio
import requests
import logging
import time


# 协程函数
async def get_request(url):
    # 进程开始计时
    process_start_time = time.time()
    response = requests.get(url)
    text = response.text
    # 进程结束时间
    process_stop_time = time.time()
    logger.info(f'结束爬虫{url},耗时{process_stop_time - process_start_time}秒')
    return text


if __name__ == "__main__":
    # 创建日志
    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(name)s %(message)s')
    # 实例化日志类
    logger = logging.getLogger()
    # 要爬取的链接
    urls = [
        'http://127.0.0.1:5000/do',
        'http://127.0.0.1:5000/la',
        'http://127.0.0.1:5000/mi'
    ]
    # 程序开始计时
    program_start_time = time.time()
    # 开始记录
    logger.info(f'程序开始')

# ------------------------------------------------------------------------------------
    # 创建协程列表
    tasks = []
    # 循环添加任务
    for url in urls:
        # 1.创建协程对象
        coro = get_request(url)
        # 2.创建任务对象
        task = asyncio.ensure_future(coro)
        # 3. 添加任务
        tasks.append(task)

    # 创建事件循环对象
    loop = asyncio.get_event_loop()
    # 必须使用wait方法对象tasks进行封装
    loop.run_until_complete(asyncio.wait(tasks))

# -----------------------------------------------------------------------
    # 结束全部
    program_stop_time = time.time()
    logger.info(f'程序结束，总耗时{program_stop_time-program_start_time}')
```

​		虽然我们用循环实现了多任务操作，但这实际上还是一个串行操作。

​	![image-20220602154557675](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220602154557675.png?lastModify=1698732446)

#### ⑤ HTTP协程实现

​	在前面的小节中，我们实际上都没有真正用到协程的优势，反而写法上更加奇怪和麻烦了。接下来，我们将正式的用协程来解决网络I/O密集型任务。

​	因为requests模块返回的Response对象不能和await一起使用，所以我们要引入新的模块`aiohttp`模块。



​	`aiohttp`模块是一个支持异步请求的库，它和`asyncio`模块配合使用，可以使我们非常方便地实现异步请求。

​	首先下载：

```python
pip install aiohttp
```

​	改用`aiohttp`模块发起请求：

```python
#!/usr/bin/env python 
# -*- coding: utf-8 -*-
# @Author  : Q
# @Time    : 2022/6/2 15:35
# @Function:
import asyncio
import aiohttp
import logging
import time

# ------------------------------------------------------------------------------------
# 协程函数
# 改用aiohttp发起请求
async def get_request(url):
    # 进程开始计时
    process_start_time = time.time()
    session = aiohttp.ClientSession()
    response = await session.get(url)
    await response.text()
    await session.close()
    # 进程结束时间
    process_stop_time = time.time()
    logger.info(f'结束爬虫{url},耗时{process_stop_time - process_start_time}秒')
    return response.text()
# ------------------------------------------------------------------------------------


if __name__ == "__main__":
    # 创建日志
    logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(name)s %(message)s')
    # 实例化日志类
    logger = logging.getLogger()
    # 要爬取的链接
    urls = [
        'http://127.0.0.1:5000/do',
        'http://127.0.0.1:5000/la',
        'http://127.0.0.1:5000/mi'
    ]
    # 程序开始计时
    program_start_time = time.time()
    # 开始记录
    logger.info(f'程序开始')

    # 创建协程列表
    tasks = []
    # 循环添加任务
    for url in urls:
        # 1.创建协程对象
        coro = get_request(url)
        # 2.创建任务对象
        task = asyncio.ensure_future(coro)
        # 3. 添加任务
        tasks.append(task)

    # 创建事件循环对象
    loop = asyncio.get_event_loop()
    # 必须使用wait方法对象tasks进行封装
    loop.run_until_complete(asyncio.wait(tasks))

    # 结束全部
    program_stop_time = time.time()
    logger.info(f'程序结束，总耗时{program_stop_time-program_start_time}')
```

![image-20220602160026027](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220602160026027.png?lastModify=1698732446)

​	从上面的运行结果中，我们可以很明显的感觉到使用协程的异步爬虫的效率要远远高于同步爬虫，虽然每个任务都还是5秒，但是几个任务之间异步运行，最终运行时间也是5秒左右。

​	这是因为开始运行时，事件循环运行第一个task，当执行到第一个await跟着的get方法时，它会被挂起，但这个get方法第一步的执行是非阻塞的，挂起之后会立马被幻想，立即又进入执行，并创建了ClientSession对象。

​	接着遇到第二个await，调用session.get请求方法，然后就被挂起了。由于请求需要耗时很久，所以一直没有被唤醒，好在第一个task被挂起了。

​	此时事件循环会寻找当前未被挂起的协程继续执行，于是转而去执行第二个task，流程操作和第一个task是一样的。以此类推，直到最后一个的task的session.get方法之后，全部的task都被挂起了。

​	所有的task都已经处于挂起状态，等待5秒之后，几个请求几乎同时有了响应，然后几个task也被唤醒接着接着执行，并输出最终结果，最后总耗时差不多就是5秒左右。

​	这就是异步操作的便捷之处，当遇到阻塞式操作时，task被挂起，程序接着去执行其他task，而不是傻傻地等待，这样就可以充分利用CPU，而不必把时间浪费在等待I/O上。

​	当然这是建立在服务器即使在同一时刻能够接收无限次请求，依然能保证正常返回结果的前提下。另外还要忽略IO传输时间延迟。有时候由于不同服务器处理task的实现机制不同，可能某些服务器并不能承受那么高的并发量，也会减慢响应速度。





## 15.2 Scrapy架构  

### 15.2.1 前言

​	在本章之前，我们大多数基于requests或aiohttp来实现爬虫的整个逻辑的。可以发现，在这个过程中，我们需要实现爬虫相关的所有操作，例如爬取逻辑、异常处理、数据解析、数据存储等，但其实这些步骤很多是通用或者重复的。既然如此，我们完全可用把这些步骤的逻辑抽离出来，把其中通用的功能做成一个个基础的组件。

​	抽离出基础组件以后，我们每次写爬虫只需要在这些组件基础上加上特定的逻辑就可以实现爬取的流程了，而不用再把爬虫每个细小的流程都实现一边。

​	比如说：我们想实现这样一个爬取逻辑，遇到403就发起重试，遇到404就直接跳过。这个逻辑其实很多爬虫都是类似的，那么我们可以把这个逻辑封装成一个通用的方法或类来直接实现。这样就大大节约了开发成本，同时在慢慢积累的过程中，这个通用的方法或类也会变得越来越健壮，从而进一步保障了项目的稳定性，框架就是基于这种思想诞生出来的。

​	同时，我们在15.1的时候也发现，异步爬虫的逻辑实现相对要比较复杂，Scrapy框架已经封装了异步的逻辑了。

**注：Scrapy框架几乎是Python爬虫学习和工作过程中必须掌握的框架，需要好好钻研和掌握。**

- 中文文档：https://www.osgeo.cn/scrapy/intro/tutorial.html
- 英文文档：https://docs.scrapy.org/en/latest/



### 15.2.2 架构设计（理解）

​	参考链接：https://www.runoob.com/w3cnote/scrapy-detail.html

#### ① 架构

​	首先从整体上来看一下Scrapy框架的架构，如下图（绿线是数据流动）：

![image-20220602171203137](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220602171203137.png?lastModify=1698732446)



- **Scrapy Engine(引擎)**: 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。
- **Scheduler(调度器)**: 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。
- **Downloader（下载器）**：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，
- **Spider（爬虫）**：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器).
- **Item Pipeline(管道)**：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。
- **Downloader Middlewares（下载中间件）**：你可以当作是一个可以自定义扩展下载功能的组件。
- **Spider Middlewares（Spider中间件）**：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）





#### ② 数据流动

1. 引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。
2. 引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。
3. 引擎向调度器请求下一个要爬取的URL。
4. 调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。
5. 一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。
6. 引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。
7. Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。
8. 引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。
9. (从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。



​	以上步骤介绍了爬虫执行过程中的数据流动过程，现在一开始看肯定是看不懂的，但无需担心，我们接下来会结合实战案例慢慢地去理解这个过程。



​	从整体上来看，各个组件都只专注于一个功能，组件和组件之间的耦合度非常低，也非常容易扩展。再由引擎将各个组件组合起来，使得各个组件各司其职，互相配合，共同完成爬取工作。另外加上Scrapy对异步处理的支持，Scrapy还可以最大限度地利用网络带宽，提高数据爬取和处理的效率。



## 15.3 Scrapy框架的使用（重点）

​	

​	 **`注意：15.3是本章的重点，掌握本节的内容就基本上够了，Python 1+X考证就考到这里`**

### 安装Scrapy

```
pip install Scrapy
```



安装好Scrapy框架之后，我们首先来了解一下Scrapy爬虫的制作过程:

**制作 Scrapy 爬虫 一共需要4步：**

1. 新建项目 (scrapy startproject xxx)：新建一个新的爬虫项目
2. 明确目标 （编写items.py）：明确你想要抓取的目标
3. 制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页
4. 存储内容 （pipelines.py）：设计管道存储爬取内容





### 实训一：百度爬虫

​	链接：https://baidu.com/



​	`【知识点：① 新建项目和项目结构 ② 编写爬虫(xxspider.py) ③ 配置settings.py】`



**【第一步：创建一个工程】**

```sh
scrapy startproject fristProject
```

![image-20220606091754173](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606091754173.png?lastModify=1698732446)

​	

​	运行完毕后，当前文件夹会生成一个名为【fristProject】的文件夹，文件夹结构如下所示：

​	![image-20220606092343278](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606092343278.png?lastModify=1698732446)

- spiders    # 放置Spiders的文件夹
  - __init__.py
- __init__.py
- item.py    # Items的定义，定义爬取的数据结构
- middleware.py     # Middlewares的定义，定义爬取时的中间件
- pipelines.py    # Pipelines的定义，定义数据管道
- settings.py    # 配置文件



**【第二步：创建Spider】**

```python
cd fristProject
scrapy genspider baidu baidu.com
```

![image-20220606093752830](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606093752830.png?lastModify=1698732446)

​	

​	执行完毕后，spiders文件夹中多了一个baidu.py，它就是刚刚创建的Spider。

![image-20220606093839207](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606093839207.png?lastModify=1698732446)

​		

这个BaiduSpider就是刚才命令行自动创建的Spider，它继承了scrapy的Spider类。

BaiduSpider有3个属性：

- name：每个项目唯一的名字，用来区分不同的Spider
- allowd_domains：允许爬取的域名，如果初始或后续的请求链接不是这个域名下的，则请求链接会被过滤掉
- start_url：包含了Spider在启动时爬取的URL列表，初始请求是由它来定义的。

BaiduSpuder有1个方法：

- parse()：在默认情况下，start_urls里面的链接构成的请求完成下载后，parse()方法就会被调用，返回的响应就会作为唯一的参数传递给parse()方法。该方法负责解析返回的响应、提取数据或者进一步生成要处理的请求，相当于回调函数。

​	

​	我们可以通过修改baidu.py来定义爬虫逻辑

​	baidu.py

```python
import scrapy


class BaiduSpider(scrapy.Spider):
    # 爬虫名的唯一标识
    name = 'baidu'

    # 允许的域名
    # ps:但是有一个问题：对于start_urls里的起始爬取页面，它是不会过滤的，它的作用是过滤首页之后的页面-----待验证
    allowed_domains = ['baidu.com']

    # 起始的url列表：只可以存储url
    # 作用：列表中存储的url都会被进行get请求的发送
    start_urls = ['https://baidu.com/']

    # 数据解析
    # parse方法调用的次数完全取决于请求的次数
    # 参数response：表示的就是服务器返回的响应对象
    def parse(self, response):
        # 输出响应
        print(response)
```

​	

![image-20220606094821429](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606094821429.png?lastModify=1698732446)



**【第三步：修改settings.py】**

​	参考链接：https://blog.csdn.net/Lan_cer/article/details/87554025

​	settings.py是整个项目的设置文件。

​	Scrapy项目允许问我们自定义所有Scrapy组件的行为，其中就是通过修改settings.py来完成。

​	我们将会在接下来的学习中一步一步的了解Scrapy组件的设置。

![image-20220606100046196](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606100046196.png?lastModify=1698732446)



Scrapy框架可以客制化的设置有很多，我们从中选3个用得比较多的基本设置开始：

（其余的大家可以参考链接 https://blog.csdn.net/Lan_cer/article/details/87554025）



1. 禁止robots：`ROBOTSTXT_OBEY = False`

   ![image-20220606100753264](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606100753264.png?lastModify=1698732446)

   

2. 指定日志类型：`LOG_LEVEL = 'ERROR'`  

   ![image-20220606101320319](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606101320319.png?lastModify=1698732446)

   执行工程后，默认会输出工程所有的日志信息（默认`LOG_LEVEL='INFO'`）。为了避免输出过多的日志信息，我们将日志等级提升到ERROR

   

3. UA伪装：`USER_AGENT = ''`

​		![image-20220606101507896](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606101507896.png?lastModify=1698732446)



**【第四步：执行爬虫】**

```sh
scrapy crawl baidu
```

![image-20220606101712095](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606101712095.png?lastModify=1698732446)





### 实训二：橙汁园首页爬虫

​	链接：http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index



​	`【知识点：① 数据解析（xpath）  ② 定义Item   ③ 定义Piplines（入门）-- 数据存储】`



**基于管道的持久化存储(重点)**

1. 在爬虫文件中进行数据解析 在items.py中定义相关属性 步骤1中解析出了几个字段的数据，在此就定义几个属性
2. 在爬虫文件中将解析到的数据存储封装到Item类型的对象中 将Item类型的对象提交给管道
3. 在管道文件（pipelines.py）中,接收爬虫文件提交过来的Item类型对象，且对其进行任意形式的 持久化存储操作
4. 在配置文件中开启管道机制



**【第一步：创建项目】**

```python
scrapy startproject lcvcProject
```



**【第二步：定义Item】**

​	Item是保存爬取数据的容器，定义了爬取结果的数据结构。它的使用方法和字典类似。不过相比字典，Item多了额外的保护机制，可以避免拼写错误或者定义字段错误。

```python
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class LcvcprojectItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()

    # 商品名
    name = scrapy.Field()
    # 商品价格
    price = scrapy.Field()
    # 库存
    stock = scrapy.Field()
```

![image-20220606111214587](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606111214587.png?lastModify=1698732446)



**【第三步：创建爬虫】**

```
cd lcvcProject
scrapy genspider lcvc 120.79.0.124
```

​	从实训一的结果我们也能看到，parse()方法接收到了start_urls里面的链接爬取后的Response。其中这个Response包含了Status、Body、Headers等内容，我们只需要对其进行数据解析，就可以提取到响应的数据了。

![image-20220606105823673](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606105823673.png?lastModify=1698732446)

​	在我们的Scrapy中，甚至已经封装好了Xpath和CSS的解析方法。

```python
response.xpath('xpath表达式')
response.css('css表达式')
```

​	我们可以通过修改lcvc.py来定义爬虫逻辑

```python
import scrapy
from lcvcProject.items import LcvcprojectItem


class LcvcSpider(scrapy.Spider):
    name = 'lcvc'
    # 将域名注释掉，就不会过滤域名
    # allowed_domains = ['120.79.0.124']

    # 初始的url列表
    start_urls = ['http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index']

    # 数据解析
    def parse(self, response):
        # 数据解析名称和内容
        div_list = response.xpath('/html/body/div/div/div[2]/div[2]/div')
        for div in div_list:

            # # 下面解析出来的内容不是字符串数据，说明和etree中的xpath使用方式不同
            # # xpath返回的列表中存储而是Selector对象，其实我们想要的字符串数据被存储在了该对象的data属性中了
            # name = div.xpath('./h3/text()')[0]
            # price = div.xpath('./label/em/text()')[0]
            # stock = div.xpath('./label/text()')[0]

            # # 将Selector对象data属性值取出
            # # extract()就是将data属性值取出（以列表形式）
            # name = div.xpath('./h3/text()').extract()
            # price = div.xpath('./label/em/text()').extract()
            # stock = div.xpath('./label/text()').extract()

            # extract_first():将列表中的第一个列表元素表示的Selector对象中的data值取出
            name = div.xpath('./h3/text()').extract_first()
            price = div.xpath('./label/em/text()').extract_first()
            stock = div.xpath('./label/text()').extract_first()

            # 输出结果
            # print(name,price,stock)

            # 实例化 LcvcprojectItem对象
            item = LcvcprojectItem()
            # 传值
            item['name'] = name
            item['price'] = price
            item['stock'] = stock
            # 将Item类型的对象提交给管道
            # 一次只能接收一次item 每接收一次调用一次
            yield item
```



**【第四步：定义Pipelines】**

​	scrapy的pipeline是一个非常重要的模块，主要作用是将return的items写入到数据库、文件等持久化模块，下面我们先简单的了解一下pipelines的用法。

​	（具体的使用在实训三）

```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter


# 文本格式进行持久化存储
class LcvcprojectPipeline:
    # 初始化
    def __init__(self):
        # 文件
        self.fp = None

    # 重写父类的两个方法
    def open_spider(self, spider):
        print('使用txt方式的open_spider(),只会在爬虫开始的时候执行一次！')

        self.fp = open('./lcvc.txt', 'w', encoding='utf-8')

    def close_spider(self, spider):
        print('使用txt方式的close_spider(),只会在爬虫结束的时候执行一次！')
        self.fp.close()

    # 该方法是用来接收item对象。一次只能接收一个item，说明该方法会被调用多次
    # 参数item：就是接收到的item对象
    def process_item(self, item, spider):
        # print(item) #item其实就是一个字典
        # 提示
        print(f'正在写入{item["name"]}')
        self.fp.write(item['name'] + ':' + item['price'] + '\t\t' + item['stock'] + '\n')
        # 将item存储到本文文件
        return item
```





**【第五步：修改settings.py】**

1. 禁止robots：`ROBOTSTXT_OBEY = False`

2. 指定日志类型：`LOG_LEVEL = 'ERROR'`  

3. UA伪装：`USER_AGENT = ''`

   

4. 开启管道：`ITEM_PIPELINES = {}`

   在第二步~第四步中已经定义了ITEM和PIPELINES，接下来我们还需要在settings.py中开启我们设置好的管道。

   找到：

   ![image-20220606114006957](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606114006957.png?lastModify=1698732446)

   取消注释，开启管道，注意这里要对应你自己设置的管道路径名

   ![image-20220606114108113](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606114108113.png?lastModify=1698732446)

​	其中300是优先级，数字值越小，优先级越高，优先级高的pipline优先被调用



**【第六步：执行爬虫】**

```sh
scrapy crawl lcvc
```



### 实训三：橙汁园首页爬虫（数据存储）

​	链接：http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index



​	`【知识点：① 定义Piplines（详解）-- 数据存储】`



**【第一步：创建项目】**

​	同实训二

**【第二步：定义Item】**

​	同实训二

**【第三步：创建爬虫】**

​	同实训二



**【第四步：定义Piplines】**

​	在实训3中，我们已经初步了解Item Pipeline的作用，在本实训中，我们会更进一步的详细了解它的用法。

​	Item Pipeline即项目管道，它的调用发生在Spider产生Item之后。当Spider解析完Response，通过`yield`关键字上传后，Item就会被Engine传递到Item Pipeline中。

​	被定义的Item Pipeline组件会根据优先级顺次被调用，完成一连串的处理过程，比如数据清洗、存储等。



Item Pipeline的主要功能如下：

1. 清洗HTML数据。
2. 验证爬取数据，检查爬取字段
3. 查重并丢弃重复内容
4. 将爬取结果存储到数据库（或文件）中。



首先我们来看一下Item Pipeline的核心构造：

```python
class SomethingPipeline(object):
    def __init__(self):    
        # 可选实现，做参数初始化等
        # 写入你的业务逻辑

    def process_item(self, item, spider):
        # item (Item 对象) – 爬取数据的item
        # spider (Spider 对象) – 爬取该item的spider
        # 这个方法必须实现，每个item pipeline组件都需要调用该方法，
        # 这个方法必须返回一个 Item 对象，被丢弃的item将不会被之后的pipeline组件所处理。
        return item

    def open_spider(self, spider):
        # spider (Spider 对象) – 被开启的spider
        # 可选实现，spider开启时，这个方法被调用。
        # 在这里我们可以做一些初始化操作，如开启数据库连接等。

    def close_spider(self, spider):
        # spider (Spider 对象) – 被关闭的spider
        # 可选实现，spider关闭时，这个方法被调用
        # 在这里我们可以做一些收尾工作，如关闭数据库连接等。
```



​	根据Item Pipeline的构造方式，我们创建了实训二中的txt文本存储方式

- pipelines.py

```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter


# 文本格式进行持久化存储
class LcvcprojectPipeline:
    # 初始化
    def __init__(self):
        # 文件
        self.fp = None

    # 重写父类的两个方法
    def open_spider(self, spider):
        print('使用txt方式的open_spider(),只会在爬虫开始的时候执行一次！')

        self.fp = open('./lcvc.txt', 'w', encoding='utf-8')

    def close_spider(self, spider):
        print('使用txt方式的close_spider(),只会在爬虫结束的时候执行一次！')
        self.fp.close()

    # 该方法是用来接收item对象。一次只能接收一个item，说明该方法会被调用多次
    # 参数item：就是接收到的item对象
    def process_item(self, item, spider):
        # print(item) #item其实就是一个字典
        # 提示
        print(f'txt正在写入{item["name"]}')
        self.fp.write(item['name'] + ':' + item['price'] + '\t\t' + item['stock'] + '\n')
        # 将item存储到本文文件
        return item
```



​	接下来我们构建一个根据json的存储方式

- pipelines.py 

  注意，我们还是在pipeline.py文件里面接着去写，别新建一个文件。

```python
import json

# JSON格式进行持久化存储
class LcvcprojectJsonPipeline:
    # 初始化
    def __init__(self):
        # 文件
        self.fp = None

    def process_item(self, item, spider):
        # 提示
        print(f'json正在写入{item["name"]}')
        # 写入文件
        json.dump(dict(item), self.fp, ensure_ascii=False, indent=4)
        # 伪装json格式
        self.fp.write(",\n")
        # 将item存储到本文文件
        return item

    # open_spider
    def open_spider(self, spider):
        print('使用json方式的open_spider()！')
        self.fp = open('./lcvc.json', 'wt', encoding='utf-8')
        # 伪装json格式
        self.fp.write('[\n')

    # close_spider
    def close_spider(self, spider):
        print('结束使用json方式的close_spider()')
        self.fp.write(']')
        self.fp.close()
```



​	按照我们在第八章学习数据存储的顺序，接下来我们就要学习数据库的存储，这也是在爬虫中非常重要的技能点，其实在经过文本格式和JSON格式的训练之后，我们接着写数据库的存储管道当然也非常简单。

​	数据库的连接是需要域名和端口的，甚至有的还需要用户名和密码，我们虽然可以直接在pipeline.py中定义，但这个方式不是Scrapy框架鼓励的。

​	Scrapy框架的设计是希望我们在settings.py中定义连接参数。

![image-20220607151627655](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220607151627655.png?lastModify=1698732446)

​	

接下来就可以通过`from_settings()`接收参数了。

```python
#  @classmethod修饰的方法是类方法，不需要实例化类就可以被类本身调用
@classmethod
def from_settings(cls, settings, *args, **kwargs):    
        '''
        from_settings()是一个类方法，用@classmethod标识，它接收一个参数crawler。
        通过crawler对象，我们可以拿到Scrapy的全局配置
        参数cls就是Class，最后返回一个Class实例。
        通过这个方法我们就可以获取settings.py中定义的数据库连接信息，并且完成数据库连接
        '''
		........
        return cls(.................)       # 返回类的实例对象，初始化
```

​	

​	我们通过一个MySQL的连接来说明`from_settings()`的作用，首先先把MySQL的连接信息定义完毕（如上图）

​	接下来来构建一个MySQL的管道：

```python
# MYSQL格式进行持久化存储
class LcvcprojectMySQLPipeline:
    
    # 初始化
    def __init__(self, mysql_configure):
        # 数据库连接
        self.mysql_configure = mysql_configure


    #  from_settings(cls, settings)
    #  @classmethod修饰的方法是类方法，不需要实例化类就可以被类本身调用
    @classmethod
    def from_settings(cls, settings):
        '''
        from_crawler是一个类方法，用@classmethod标识，它接收一个参数crawler。
        通过crawler对象，我们可以拿到Scrapy的全局配置
        参数cls就是Class，最后返回一个Class实例。
        通过这个方法我们就可以获取settings.py中定义的数据库连接信息，并且完成数据库连接
        '''
        # mysql配置信息
        mysql_configure = {}
        # 从settings.py中获取MySQL连接信息
        mysql_configure['host'] = settings.get('MYSQL_HOST')
        mysql_configure['port'] = settings.get('MYSQL_PROT')
        mysql_configure['user'] = settings.get('MYSQL_USER')
        mysql_configure['password'] = settings.get('MYSQL_PASS')
        mysql_configure['db'] = settings.get('MYSQL_DB')
        # 初始化配置信息
        return cls(mysql_configure)

    # 开启方法
    def open_spider(self, spider):
        # 提示
        print('使用mysql方式的open_spider()！')
        # 连接数据库
        self.conn = pymysql.connect(**self.mysql_configure)
        # 创建数据库游标
        self.cursor = self.conn.cursor()
        # 建表
        sql = '''
            CREATE TABLE IF NOT EXISTS LCVC_MENUS(
                FOOD_NAME VARCHAR(255) NOT NULL UNIQUE ,
                PRICE FLOAT,
                STOCK VARCHAR(255)
            );
        '''

        # 建表
        self.cursor.execute(sql)


    # 进程
    def process_item(self, item, spider):
        # 爬虫进程
        sql = f'''
            INSERT INTO LCVC_MENUS(FOOD_NAME,PRICE,STOCK)VALUES (
                "{item['name']}",{float(item['price'])},"{item['stock']}"
            );
        '''
        try:
            self.cursor.execute(sql)
            self.conn.commit()
        except Exception as e:
            print(e)
            self.conn.rollback()
        else:
            print(f"MYSQL成功存储{item['name']}")

        finally:
            return item


    def close_spider(self, spider):
        # 关闭mysql
        print('结束使用mysql方式的close_spider()')
        # 关闭连接
        self.conn.close()
```

​				**请同学们自己去完成redis的存储**



**【第五步：修改settings.py】**

1. 禁止robots：`ROBOTSTXT_OBEY = False`

2. 指定日志类型：`LOG_LEVEL = 'ERROR'`  

3. UA伪装：`USER_AGENT = ''`

   

4. 开启管道：`ITEM_PIPELINES = {}`

   在第二步~第四步中已经定义了ITEM和PIPELINES，接下来我们还需要在settings.py中开启我们设置好的管道。

   找到：

   ![image-20220606114006957](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220606114006957.png?lastModify=1698732446)

   取消注释，开启管道，注意这里要对应你自己设置的管道路径名

   ![image-20220607160207737](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220607160207737.png?lastModify=1698732446)

​	其中300，200，100是优先级，数字值越小，优先级越高，优先级高的pipline优先被调用。

​	也就是先调用MYSQL管道，然后是JSON管道，最后才是TXT管道。



**【第六步：执行爬虫】**

```sh
scrapy crawl lcvc
```



### 实训四：段子网多页爬虫

​	链接：https://duanzixing.com/



```
【知识点：xxspider.py手动请求发送 -- ① 分页爬取  ② POST请求】
```



​	在前面三个实训中，我们是将要爬取的链接全部放到 ![image-20220607161014835](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220607161014835.png?lastModify=1698732446)中，让Scrapy自动发起请求，我们并没有深究其原理。

​	其次，我们之前的Scrapy爬虫中，基本都是只爬取一页。

​	虽然说我们也可以把所有要爬取的页面全部放到start_urls中，如下：

```python
start_urls = [f'https://duanzixing.com/page/{page}/' for page in range(1,11)]
```

​	这样的方法存在局限性，它只能爬取我们一开始规定好的页面。

​	再者，如果请求是POST请求的话，我们上述的做法就失败了。

​	接下来我们就来一起探究一下Scrapy的请求原理，自己去手动构造请求，来打破上述提到的局限性。



【知识点一】

​	在Scrapy框架中，GET请求发送的方式是：

```python
	# callback指定解析函数，用于解析数据
    # GET请求
	yield scrapy.Request(url, callback)
    
    # # 完整版
    # yield scrapy.Request(url, headers. cookies, meta, callback=self.parse_response)
```

POST请求的方式是：

```python
	# POST请求
	yield scrapy.FormRequest(url,callback,formdata)
    
    # # 完整版
    # yield scrapy.FormRequest(url, headers. cookies, formdata, meta, callback=self.parse_response)
```

​	

​	Scrapy框架的请求是以一个生成器的方法发起请求，我们在之前的学习中也接触过生成器。它是将所有Request加入到队列中。

​	Engine之后会按照队列的顺序交给Downloader进行处理，这点我们可以不用关心。



​	在`scrapy.Request(url, callback)`还有一个必须用到的参数callback，它默认是`self.parse()`，代表回调函数。代表将Request执行请求后得到的Response对象调用到该回调函数中。

​	这也是为什么我们能在`self.parse()`进行数据解析的原因。

​	

【知识点二】

​	在Scrapy框架中，已经默认为我们实现一个`start_requests()`方法：

```python
def start_requests(self):
    for u in self.start_urls:
        yield Request(u,callback=self.parse)
```

​	其中的逻辑就是读取start_urls然后生成Request，并为Request指定回调函数(callback)，在Response回调到`self.parse()`中。

​	这也就是为什么我们可以在`self.parse()`中解析Response的原因

​	我们还发现这是一个生成器，返回的所有Request都会作为初始Request加入调度队列。



​	因此，如果我们想要自定义初始请求，只需要重新`start_requests()`方法就可以。比如说我们改成POST请求。

```python
def start_requests(self):
 	for u in self.start_urls:
		yield scrapy.FormRequest(url=u,callback=self.parse)
```



​	我们开始正式的段子网爬虫实训：

**【第一步：创建一个工程】**

```
scrapy startproject duanziProject
```

**【第二步：定义Item】**

​	items.py

```python
import scrapy


class DuanziprojectItem(scrapy.Item):
    # define the fields for your item here like:

    # 标题
    title = scrapy.Field()
    # 作者
    author = scrapy.Field()
    # 时间
    re_time = scrapy.Field()
    # 正文
    content = scrapy.Field()
```



**【第三步：创建爬虫】**

```python
cd duanziProject
scrapy genspider duanzi duanzixing.com
```

​	duanzi.py

```python
import scrapy
from duanziProject.items import DuanziprojectItem

class DuanziSpider(scrapy.Spider):
    name = 'duanzi'
    # allowed_domains = ['https://duanzixing.com/']
    start_urls = ['https://duanzixing.com//']

    # 通用的url模板
    base_url = 'https://duanzixing.com/page/{}/'
    # 页码（从第二页开始）
    page = 2

    # 重写父类方法：这个是该方法的原始实现
    # 这里其实和原本一样
    def start_requests(self):
        for u in self.start_urls:
            yield scrapy.Request(url=u, callback=self.parse)

    # 将段子网中所有页码对应的数据进行爬取
    def parse(self, response):
        # 找到所有的article
        article_ls = response.xpath('/html/body/section/div/div/article')
        # 遍历
        for article in article_ls:
            # 实例化item
            item = DuanziprojectItem()
            # 标题
            item['title'] = article.xpath('./header/h2/a/text()').extract_first()
            # 作者
            item['author'] = article.xpath('./p[1]/span[1]/text()').extract_first()
            # 时间
            item['re_time'] = article.xpath('./p[1]/time/text()').extract_first()
            # 正文
            item['content'] = article.xpath('./p[2]/text()').extract_first()
            # 传递给管道
            yield item

        # 对新的页面发起请求
        # 直到递归条件结束
        if self.page < 11:  # 结束递归的条件
            print(self.page)
            new_url = self.base_url.format(self.page)  # 其他页码对应的完整url
            self.page += 1
            # 对新的页码对应的url进行请求发送（手动请求GET发送）
            yield scrapy.Request(url=new_url, callback=self.parse)
```

​	我们在`self.parse()`后面手动设置了一个对新的url发起的请求。

​	由于回调函数还是`self.parse()`，当递归条件未结束时，Scrapy爬虫会一直重复调用`self.parse()`。

​	此时`self.parse()`兼顾了数据解析和发送请求的功能



**【第四步：定义Piplines】**

```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter
import json

# JSON格式进行持久化存储
class DuanziprojectPipeline:
    # 初始化
    def __init__(self):
        # 文件
        self.fp = None

    def process_item(self, item, spider):
        # 提示
        print(f'JSON正在写入{item["title"]}')
        # 写入文件
        json.dump(dict(item), self.fp, ensure_ascii=False, indent=4)
        # 伪装json格式
        self.fp.write(",\n")
        # 将item存储到本文文件
        return item

    # open_spider
    def open_spider(self, spider):
        print('使用json方式的open_spider()！')
        self.fp = open('./duanzi.json', 'wt', encoding='utf-8')
        # 伪装json格式
        self.fp.write('[\n')

    # close_spider
    def close_spider(self, spider):
        print('结束使用json方式的close_spider()')
        self.fp.write(']')
        self.fp.close()
```



**【第五步：修改settings.py】**

1. 禁止robots：`ROBOTSTXT_OBEY = False`

2. 指定日志类型：`LOG_LEVEL = 'ERROR'`  

3. UA伪装：`USER_AGENT = ''`

   

4. 开启管道：`ITEM_PIPELINES = {}`



**【第六步：执行爬虫】**

```
scrapy crawl duanzi
```





### 实训五：远鉴字幕组电影详情页爬虫

​	链接：https://ifenpaidy.com/blog



​	`【知识点：请求传参--详情页的爬取】`



​	在完成了实训四的训练之后，我们已经掌握了手动发起请求的方式，那么实际上我们不仅可以对分页进行请求，除此之外我们还可以对详情页发起请求。

​	比如远鉴字幕组的电影网站，详细的数据都在详情页中：

 ![image-20220607172813676](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220607172813676.png?lastModify=1698732446)    ![image-20220607172850462](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220607172850462.png?lastModify=1698732446)



​	但是现在又有一个问题，我们即使可以通过`yield Request(详情页)`向详情页发起请求，但是item也是通过`yield`方式传递给管道，这两个之间产生了矛盾。

![image-20220607172949069](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220607172949069.png?lastModify=1698732446)

​	

​	此时我们要用到一个`yield Request()`的参数 meta

```python
	# callback指定解析函数，用于解析数据
    # GET请求
	yield scrapy.Request(url, callback, meta)
```

​	meta是Request请求携带的额外参数，利用meta，我们可以向回调方法传递信息。

​	也就是说，我们可以把首页中的item通过参数meta传递到详情页的请求中。

​	除此之外，我们还得构造一个专门针对于详情页的解析回调方法`self.parse_detail()`



**【第一步：创建一个工程】**

```
scrapy startproject movieProject
```



**【第二步：定义Item】**

​	items.py

```python
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class MovieprojectItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # 标题
    title = scrapy.Field()
    # 详情页链接
    detail_url = scrapy.Field()
    # 导演（详情页特有）
    direct = scrapy.Field()
    
    # 其他的请同学们自己去完善了，实训手册里面就不写了
```



**【第三步：创建爬虫】**

```
cd movieProject
scrapy genspider movie ifenpaidy.com
```

​	movie.py

```python
import scrapy
from movieProject.items import MovieprojectItem

class MovieSpider(scrapy.Spider):
    name = 'movie'
    # allowed_domains = ['ifenpaidy.com/']
    # 模板
    base_url = 'https://ifenpaidy.com/blog?page={}'
    # 起始url
    start_urls = [base_url.format(1)]
    # 页码
    page = 2

    def parse(self, response):
        # 定位所有的div_ls
        div_ls = response.xpath('//*[@id="body"]/div[3]/div/div[1]/div/div')
        # 遍历
        for div in div_ls:
            # 实例化item
            item = MovieprojectItem()
            # 标题
            item['title'] = div.xpath('./h2/a/text()').extract_first()
            # 详情页链接
            detail_url = 'https://ifenpaidy.com' + div.xpath('./h2/a/@href').extract_first()
            item['detail_url'] = detail_url

            # meta参数：可以将meta字典里面的内容传给callback
            meta = {'item': item}


            # 构造向详情页发起的请求
            yield scrapy.Request(url=detail_url, callback=self.parse_detail, meta=meta)

        # 分页爬取 前十页
        if self.page < 11:
            print(self.page)
            new_url = self.base_url.format(self.page)
            self.page += 1
            yield scrapy.Request(url=new_url, callback=self.parse)

    # 定义详情页的解析方式
    def parse_detail(self, response):
        # 提取传过来的item
        item = response.meta['item']
        # 解析详情页
        item['direct'] = response.xpath('//div[@class="title-break"]/p[1]/text()')
        # 将item传给管道
        yield item
```

​	在movieSpider中我们加入了对详情页的请求，并且通过meta参数将首页获取的item传递给详情页，最终再传递给管道



**【第四步：定义Piplines】**

```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter
import json

# JSON格式进行持久化存储
class MovieprojectPipeline:
    # 初始化
    def __init__(self):
        # 文件
        self.fp = None

    def process_item(self, item, spider):
        # 提示
        print(f'JSON正在写入{item["title"]}')
        # 写入文件
        json.dump(dict(item), self.fp, ensure_ascii=False, indent=4)
        # 伪装json格式
        self.fp.write(",\n")
        # 将item存储到本文文件
        return item

    # open_spider
    def open_spider(self, spider):
        print('使用json方式的open_spider()！')
        self.fp = open('./movie.json', 'wt', encoding='utf-8')
        # 伪装json格式
        self.fp.write('[\n')

    # close_spider
    def close_spider(self, spider):
        print('结束使用json方式的close_spider()')
        self.fp.write(']')
        self.fp.close()
```

**【第五步：修改settings.py】**

1. 禁止robots：`ROBOTSTXT_OBEY = False`

2. 指定日志类型：`LOG_LEVEL = 'ERROR'`  

3. UA伪装：`USER_AGENT = ''`

   

4. 开启管道：`ITEM_PIPELINES =` 



**【第六步：执行爬虫】**

```
scrapy crawl movie
```



## 15.4 Scrapy框架的进阶使用（更新中）

​	在15.3中，我们通过完成了五个实训掌握了Scrapy框架的使用，在熟练Scrapy框架的使用之后，我们可以更简便的构造出一个高性能高强壮性的爬虫。

​	其实Scrapy框架的功能还远远不止于此，比如说在【架构设计】中我们介绍到的Middlewares还没用上。

![image-20220602171203137](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220602171203137.png?lastModify=1698732446)

​	除此之外，Scrapy框架还支持基于链接的深度爬取，以及支持分布式爬虫（15.6），对下载图片也有封装的管道类，还有很多功能非常强大的拓展包（Extension）。

​	

### 实训六：橙汁园图片爬虫

​	链接：http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index



```
【知识点： ① Scrapy下载图片】
```

​	

​	在之前的学习中，我们了解到了可以通过管道去存储数据。如果我们想下载图片，我们就可以去定义一个下载图片的管道即可。

​	在Scrapy框架中，其实不用那么麻烦，它专门封装了几个专门用于图片，视频，文件下载的大文件管道类

```python
from scrapy.pipelines.images import ImagesPipeline  #图片下载模块
from scrapy.pipelines.files import FilesPipeline
from scrapy.pipelines.media import MediaPipeline
```

​	我们可以继承它们来定义我们的大文件管道，就可以很高效的去下载大文件了。



具体流程如下：

```
- 下属管道类是scrapy封装好的我们直接用即可：
    1. from scrapy.pipelines.images import ImagesPipeline #提供了数据下载功能
    - 重写该管道类的三个方法：
    2. get_media_requests
        - 对图片地址发起请求
    3. file_path
        - 返回图片名称即可
    4. item_completed
        - 返回item，将其返回给下一个即将被执行的管道类
    5. 在配置文件中添加：
        - IMAGES_STORE = 'dirName'
```









**【第一步：创建一个工程】**

```python
scrapy startproject imgPro
```

**【第二步：定义Items】**

​	items.py

```python
import scrapy


class ImgproItem(scrapy.Item):
    # define the fields for your item here like:
    # 图片名
    name = scrapy.Field()
    # 图片链接
    src = scrapy.Field()
```



**【第三步：创建爬虫】**

```python
cd imgPro
scrapy genspider img 120.79.0.124:8080
```

​	img.py

```python
import scrapy

from imgPro.items import ImgproItem


class ImgSpider(scrapy.Spider):
    name = 'img'
    # allowed_domains = ['120.79.0.124:8080']
    start_urls = ['http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index']

    def parse(self, response):
        # 找到所有的新品
        div_ls = response.xpath('/html/body/div/div/div[2]/div[2]/div')
        # 遍历新品
        for div in div_ls:
            # 实例化item
            item = ImgproItem()
            # 名字
            item['name'] = div.xpath('./h3/text()').extract_first()
            # 链接
            item['src'] = 'http://120.79.0.124:8080' + div.xpath('./a/img/@src').extract_first()
            # 传递给管道
            yield item
```



**【第四步：定义Piplines】**

​	pipelines.py

```python
from itemadapter import ItemAdapter
import scrapy
from scrapy.pipelines.images import ImagesPipeline  # 图片管道类
from scrapy.pipelines.files import FilesPipeline    # 文件管道类
from scrapy.pipelines.media import MediaPipeline    # 媒体管道类


# 继承图片管道类
class ImgproPipeline(ImagesPipeline):

    # 向图片链接发起请求
    def get_media_requests(self, item, info):
        # 输出提示
        print(f'正在请求{item["name"]}')
        # 发起请求
        yield scrapy.Request(url=item['src'], meta={'item': item})

    # 图片路径名
    def file_path(self, request, response=None, info=None, *, item=None):
        # 根据request获取meta
        # 注意：这里的内部实现和self.parse()不一样
        item = request.meta['item']
        file_path = item['name'] + '.jpg'
        # 将图片路径名传递给管道
        return file_path

    # 将item传递给下一个即将被执行的管道类
    def item_completed(self, results, item, info):
        return item
```

​	在这个图片下载管道中，我们继承了`ImagesPipeline类`，借助它的实现在加上修改一些逻辑，就可以高效简便的完成图片下载。



**【第五步：修改settings.py】**

1. 禁止robots：`ROBOTSTXT_OBEY = False`

2. 指定日志类型：`LOG_LEVEL = 'ERROR'`  

3. UA伪装：`USER_AGENT = ''`

   

4. 开启管道：`ITEM_PIPELINES =` 

   

5. **设置大文件保存目录**：`IMAGES_STORE ='./imgLibs'`

![image-20220608092356938](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220608092356938.png?lastModify=1698732446)

​		



**【第六步：执行爬虫】**

```
scrapy crawl img
```

![image-20220608092451865](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220608092451865.png?lastModify=1698732446)







### 实训七：下载器中间件爬虫

​	ps：懒得想名字了。

​	链接：



```
【知识点：① 下载器中间件（Downloader Middlewares）】
```

​	

​	接下来我们将在实训七中来介绍两个中间件（Middlewares），也就是下载器中间件和爬虫中间件。



![image-20220608095117487](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220608095117487.png?lastModify=1698732446)



​	中间件是Scrapy里面的一个核心概念。使用中间件可以批量拦截请求和响应，并且还可以对请求和响应进行定制化的修改，从而开发出适应不同情况的爬虫。

​	Scrapy框架的中间件主要有两个：

- SpiderMiddleware（爬虫中间件）
- DownloaderMiddleware（下载器中间件）

通常情况下，下载器中间件就已经能处理后请求对象和响应对象数据，所以**推荐使用下载器中间件**，一般不会去使用爬虫中间件。

因此我们首先来介绍**下载器中间件**：

![image-20220608095407305](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220608095407305.png?lastModify=1698732446)

​	可以从上图中看出，**Downloader Middleware(下载器中间件)**是处于Scrapy框架的**Engine(引擎)**和**Downloader(下载器)**之间的处理模块。

​	在 **Engine(引擎)**把从**Scheduler(调度器)**获取的Request发送给**Downloader(下载器)**的过程中； **Downloader(下载器)**把Response经过**Engine(引擎)**发送回**Spider(爬虫)**的过程中，Request和Response都会经过**Downloader Middleware(下载器中间件)**的处理。



也就是说**Downloader Middleware(下载器中间件)**在整个架构中起作用的位置是以下两个：

1. 拦截修改Request（执行下载之前）

   - 篡改请求url
   - 伪装请求头信息
     - UA
     - Cookie
   - 设置请求代理

   

2. 拦截修改Response（执行解析之前）

   - 篡改响应数据



**【第一步：创建一个工程】**

```python
scrapy startproject downloadMidPro
```



**【第二步：定义items】**

​			略



**【第三步：创建爬虫】**

```
cd downloadMidPro
scrapy genspider downloadMid www.baidu.com
```

​	downloadMid.py

```python
import scrapy


class DownloadmidSpider(scrapy.Spider):
    name = 'downloadMid'
    # allowed_domains = ['www.baidu.com']
    start_urls = ['https://www.baidu.com/']

    def parse(self, response):
        print(response)
        print(response.status)
        # print(response.body)
        pass
```

​	

​	我们在前面几步的代码稍微简单一些，重点在等下要介绍的中间件



**【第四步：修改middlewares.py】**

​	在完成实训前，我们先要对下载器中间件的知识点做一个梳理。



【知识点一：使用说明】

​	需要说明的是，Scrapy已经提供了许多Downloader Middleware，比如负责失败重试、自动重定向等功能的Downloader Middleware。

​	他们被 DOWNLOADER_MIDDLEWARES_BASE变量所定义。

- settings.py

```python
DOWNLOADER_MIDDLEWARES_BASE = {
    'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,
    'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,
    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,
    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400,
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550,
    'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,
    'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,
    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,
    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,
    'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,
    'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,
}
```

​	在默认情况下。Scrapy已经为我们开启了DOWNLOADER_MIDDLEWARES_BASE所定义的Downloader Middleware。

​	比如RetryMiddleware带有自动重试功能，RedirectMiddleware带有自动处理重定向功能。



​	我们用户也可以自定义Downloader Middleware，它的开启也是在settings.py中。

```python
DOWNLOADER_MIDDLEWARES = {
   'downloadMidPro.middlewares.DownloadmidproDownloaderMiddleware': 543,
}
```

​	开启之前，首先我们得了解Downloader Middleware是如何实现的。





【知识点二：Downloader Middleware的核心方法】

​	来到middlewares.py，我们可以在这个代码中去定义中间件（下载器中间件和爬虫中间件）。

![image-20220608112001464](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220608112001464.png?lastModify=1698732446)

​	

​	虽然看起来很复杂，但是对于Downloader Middleware，我们只需要修改3个核心方法：

1. **process_request(request, spider)**

   ![image-20220608112416870](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220608112416870.png?lastModify=1698732446)

   功能：拦截所有的正常请求

   ​			 在Request发送给Downloader之前，`process_request()`就会被调用。

   

   参数：request: Request对象，即被拦截到的请求 		 spider: Spider对象，即此请求所对应的爬虫类实例

   

   返回值：必须是 None 、Response对象、Request对象三者之一，或者抛出IgnoreRequest

   ​			    返回None时，继续执行转交给下一步中间件，直到转到Downloader

   ​				 返回Response对象时， 直接调用中间件的process_response()，交给spider

   ​				 返回Request对象时，修改请求交给调度器，等待重新被调用。即重新发送一个新请求。

   ​				 抛出IgnoreRequest时，顺序执行process_exception(),如未得到处理,errorback()回调。如还未处理，将忽略。

   

   

2. **process_response(request, response, spider)**

   ![image-20220608113339131](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220608113339131.png?lastModify=1698732446)

   功能：拦截所有的响应

   

   参数：response:拦截到的响应对象

   ​			 request:响应对象所对应的请求

   ​			 spider:响应对象所对应的爬虫实例

   

   返回值：必须是Request对象、Response对象两者之一，或者抛出IgnoreRequest

   ​				 返回Request时，重新发起请求，提交新请求到调度器中等待调用。

   ​				 返回Response，继续按顺序执行转交到下一步process_response()，直到转交给spider。这一步我们就可以修				 改响应了

   ​				 抛出IgnoreRequest时，和之前的情况一样。

   

3. **process_exception(request, exception, spider)**

​		 ![image-20220608114545945](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220608114545945.png?lastModify=1698732446)

​		功能：拦截所有异常的请求



​		参数：request: 拦截到的异常Request对象

​					 exception: Exception对象，即抛出的异常

​					 spider: 异常Request对象对应的spider



​		返回值：必须为None、Response、Request对象三者之一

​						返回None时，不做任何处理，转交到下一步的process_exception()，直到调用完毕

​						返回Response对象，返回构造的响应，顺次转交到process_response()上，直到完毕转到spider.py

​						返回Request对象，重新发起请求，提交新请求到调度器中等待调用。

​		

​		**注意：这个异常只包括下载处理器(download handler)或 `process_request()` (下载中间件)抛出的异常(包括 `IgnoreRequest` 异常)。**





​	三个方法的调用顺序



![流程图](https://img-blog.csdnimg.cn/ad948409a9a44207af1f5ee3d8a2ff3c.png)



​	

​	以上内容就是这3个方法的详细使用逻辑。

​	当然上面的内容直接看是难以理解的，下面我们结合几个例子来加深对Downloader Middleware的认识

#### ① 中间件伪装请求头

​	 通常是拦截请求即修改process_request()

```python
import random
class DownloadmidproDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    USER_AGENT_LIST = [
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1",
        "Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11",
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6",
        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6",
        "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5",
        "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5",
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
        "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
        "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
        "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
    ]


    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        '''
        拦截请求
        :param request: Request对象，即被拦截到的请求
        :param spider: Spider对象，即此请求所对应的爬虫类实例
        :return: 必须是 None 、Response对象、Request对象三者之一
                 或者抛出IgnoreRequest
        '''
        print('正在执行process_request()')
        # 伪装UA
        request.headers['User-Agent'] = random.choice(self.USER_AGENT_LIST)
        # 这里也可以伪装Cookie
        # request.header['Cookie'] = 'xxxxxxxxxx'

        return None # or request

    def process_response(self, request, response, spider):
        '''
        拦截响应
        :param request:响应对象所对应的请求（非拦截）
        :param response:拦截到的响应对象
        :param spider:响应对象所对应的爬虫实例
        :return: 必须是Request对象、Response对象两者之一，或者抛出IgnoreRequest
        '''
        return response

    def process_exception(self, request, exception, spider):
        '''
        拦截异常请求
        :param request: 拦截到的异常Request对象
        :param exception: Exception对象，即抛出的异常
        :param spider: 异常Request对象对应的spider
        :return: 必须为None、Response、Request对象三者之一
        '''
        pass

    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)
```

​	最终第六步运行之后的结果

![image-20220608121530274](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220608121530274.png?lastModify=1698732446)

#### ② 中间件伪装代理IP

​	伪装代理IP通常是遇到异常请求的时候，即修改process_exception()

```python
class DownloadmidproDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        '''
        拦截请求
        :param request: Request对象，即被拦截到的请求
        :param spider: Spider对象，即此请求所对应的爬虫类实例
        :return: 必须是 None 、Response对象、Request对象三者之一
                 或者抛出IgnoreRequest
        '''
        return None 

    def process_response(self, request, response, spider):
        '''
        拦截响应
        :param request:响应对象所对应的请求（非拦截）
        :param response:拦截到的响应对象
        :param spider:响应对象所对应的爬虫实例
        :return: 必须是Request对象、Response对象两者之一，或者抛出IgnoreRequest
        '''
        return response

    def process_exception(self, request, exception, spider):
        '''
        拦截异常请求
        :param request: 拦截到的异常Request对象
        :param exception: Exception对象，即抛出的异常
        :param spider: 异常Request对象对应的spider
        :return: 必须为None、Response、Request对象三者之一
        '''
        # 请求的ip被禁掉，该请求就会变成一个异常的请求
        # 设置代理
        request.meta['proxy'] = 'http://113.124.95.78'  
        print('process_exception()')
        # 将异常的请求修正后将其进行重新发送
        return request  

    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)
```

#### ③ 中间件篡改响应对象

​	通常我们篡改响应对象的情况很少，但如果要，就去修改process_response()

```python
class DownloadmidproDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        '''
        拦截请求
        :param request: Request对象，即被拦截到的请求
        :param spider: Spider对象，即此请求所对应的爬虫类实例
        :return: 必须是 None 、Response对象、Request对象三者之一
                 或者抛出IgnoreRequest
        '''
        return None

    def process_response(self, request, response, spider):
        '''
        拦截响应
        :param request:响应对象所对应的请求（非拦截）
        :param response:拦截到的响应对象
        :param spider:响应对象所对应的爬虫实例
        :return: 必须是Request对象、Response对象两者之一，或者抛出IgnoreRequest
        '''
        # 篡改响应
        response.status = 201
        return response

    def process_exception(self, request, exception, spider):
        '''
        拦截异常请求
        :param request: 拦截到的异常Request对象
        :param exception: Exception对象，即抛出的异常
        :param spider: 异常Request对象对应的spider
        :return: 必须为None、Response、Request对象三者之一
        '''
        pass

    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)
```

​	

​	运行结果：

![image-20220608122443158](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220608122443158.png?lastModify=1698732446)



**【第四步：定义Piplines】**

​				   略

**【第五步：修改settings.py】**

1. 禁止robots：`ROBOTSTXT_OBEY = False`
2. 指定日志类型：`LOG_LEVEL = 'ERROR'`  
3. 开启下载器中间件：`DOWNLOADER_MIDDLEWARES =` 

​		![image-20220608121243512](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220608121243512.png?lastModify=1698732446)

**【第六步：执行爬虫】**

```
scrapy crawl downloadMid
```



### 实训八：携带cookie爬取雪球网数据

​	雪球网链接：https://xueqiu.com/

​	我们在第十章的时候爬取过雪球网中的咨询信息，了解到这个网站需要不需要登陆，但会检查用户的Cookie信息。

​	其次我们也知道有些网站还要求登陆状态的cookie，这都指名我们需要进行携带Cookie和模拟登陆。

​	Scrapy框架中伪装cookie的位置可以有很多地方，可以在爬虫组件中伪装，可以在中间件里伪装，它是非常自由的框架，

接下来的实训案例只是提供一种思路。

​	关于cookie设置的其他思路可参考附录五。



**【第一步：创建一个工程】**

```python
scrapy startproject xueqiuProject
```



**【第二步：定义item】**

​	items.py

```python
import scrapy


class XueqiuprojectItem(scrapy.Item):
    # id
    id = scrapy.Field()
    # description
    description = scrapy.Field()
    # author
    author = scrapy.Field()
```



**【第三步：创建爬虫】**

```
cd xueqiuProject
scrapy genspider xueqiu xueqiu.com
```

​	xueqiu.py

```python
import scrapy
from scrapy.http.response.html import HtmlResponse
import random

from xueqiuProject.items import XueqiuprojectItem

# 常用UA，其实这里最好放到settings中，在爬虫类中用from_settings()方法取出
USER_AGENT = [
    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',
    'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16',
    'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36 Core/1.77.119.400 QQBrowser/10.9.4817.400'
]


class XueqiuSpider(scrapy.Spider):
    # 伪装的cookie
    cookie = ''
    # 唯一标识符
    name = 'xueqiu'
    # allowed_domains = ['xueqiu.com']
    start_urls = ['https://xueqiu.com/']
    # ajax链接模板
    # max_id = 358975 - size * n
    base_url = 'https://xueqiu.com/statuses/hot/listV2.json?since_id=-1&max_id={}&size=15'

    # 页码
    page = 2

    # 重写
    def start_requests(self):
        for u in self.start_urls:
            yield scrapy.Request(url=u, headers={'User-Agent': random.choice(USER_AGENT)}, callback=self.get_cookie_parse)


    # 处理 cookie 的函数
    def cookie_list_to_str(self, cookie_list):
        '''
        :param cookie_list: 提取到的cookie列表
        :return: 格式化的cookie
        '''
        # 格式化的cookie
        cookie = []
        for c in cookie_list:
            cookie.append(str(c, 'utf-8').split(';')[0])
        # 拼接
        cookie = '; '.join(cookie)
        # 返回格式化结果
        return cookie

    # 获取cookie信息
    # 接着发起请求
    def get_cookie_parse(self, response):
        # 获取Set-Cookie
        cookie_list = response.headers.getlist('Set-Cookie')
        # 整理成可用格式
        self.cookie = self.cookie_list_to_str(cookie_list)

        # 链接
        url = self.base_url.format(358975)
        # 发送ajax请求
        yield scrapy.Request(url, headers={
            'User-Agent': random.choice(USER_AGENT), 'Cookie': self.cookie}, callback=self.parse)


    def parse(self, response):
        # 响应类型的json格式
        json_data = response.json()
        # items
        items = json_data.get('items')
        # 遍历
        for i in items:
            # 实例化XueqiuprojectItem
            item = XueqiuprojectItem()
            # id
            item['id'] = i['id']
            # description
            item['description'] = i['original_status']['description']
            # author
            item['author'] = i['original_status']['user']['screen_name']

            # 传给管道
            yield item

        # 携带cookie发送请求
        # 设置递归条件
        if self.page < 1000:
            # 补全链接
            url = self.base_url.format(358975-self.page * 15)
            print(url)
            # 递归增加
            self.page += 1
            # 发送ajax请求
            yield scrapy.Request(url, headers={
                'User-Agent': random.choice(USER_AGENT), 'Cookie': self.cookie}, callback=self.parse)
```

​	其实在spider.py中设置一次cookie其实是不保险的，更好的方法是使用`cookiejar`或者在`中间件中添加cookie`，具体的实现可以去查看附录五。



**【第四步：定义Piplines】**

```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter
import json

class XueqiuprojectPipeline:
    # 初始化
    def __init__(self):
        # 文件
        self.fp = None

    def process_item(self, item, spider):
        # 提示
        print(f'JSON正在写入{item["id"]}')
        # 写入文件
        json.dump(dict(item), self.fp, ensure_ascii=False, indent=4)
        # 伪装json格式
        self.fp.write(",\n")
        # 将item存储到本文文件
        return item

    # open_spider
    def open_spider(self, spider):
        print('使用json方式的open_spider()！')
        self.fp = open('./xueqiu.json', 'wt', encoding='utf-8')
        # 伪装json格式
        self.fp.write('[\n')

    # close_spider
    def close_spider(self, spider):
        print('结束使用json方式的close_spider()')
        self.fp.write(']')
        self.fp.close()
```



​	**【第五步：修改settings.py】**

1. 禁止robots：`ROBOTSTXT_OBEY = False`

2. 指定日志类型：`LOG_LEVEL = 'ERROR'`  

3. UA伪装：`USER_AGENT = ''`

4. 开启管道：`ITEM_PIPELINES =` 

   

5. 不用默认cookie`COOKIES_ENABLED = False`



**【第六步：执行爬虫】**

```python
scrapy crawl xueqiu
```



### 实训九：深度(规则)爬虫全站爬取流言板

​		链接：https://bj.lianjia.com/ershoufang/rs/



​		`【知识点：scrapy规则化全站爬虫-- ①crawlSpider ②rule】`



​	在以往的爬虫实训中，我们都需要指定爬取的链接（比如前二十页），定义特点的方法完成请求解析这一系列的过程。在Scrapy框架中也是一样，我们通过定义一个`Spider`类，来完成这一系列的逻辑。

![image-20220609112037272](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220609112037272.png?lastModify=1698732446)

​	试想，如果我们现在要实现一个全站爬取，比如爬取新闻网的全部新闻内容。有时候我们是无法准确的知晓多少分页，有多少个详情页，存在非常非常多的链接。

​	其次，如果我们还想对非常多站点的爬取，那么可能还需要为每个站点单独创建一个`Spider`，然后在每个`Spider`里面定义请求解析的逻辑代码，但实际上这些代码实现思路是差不多的，可能还包含很多重复代码。

​	

​	这时候Scrapy框架提供了一个爬虫类`crawlSpider`，它是`Spider`的子类，利用它我们可以指定针对站点的爬虫规则，我们只需要指定站点中哪些链接需要继续爬取，符合这个规则的链接全都要。这样就不需要考虑到底有多少页了。

​	同时，我们在管理爬虫的时候，就只需要单独修改这些规则就可。



**【第一步：创建一个工程】**

```python
scrapy startproject crawlLianjiaPro
```



**【第二步：定义item】**

​	items.py

```python
import scrapy


class CrawllianjiaproItem(scrapy.Item):
    # define the fields for your item here like:
    # 标题
    title = scrapy.Field()
    # 这里偷下懒，其他的不找了，同学们自己可以去找
```



**【第三步：创建深度爬虫】**

​	这里我们创建的是深度爬虫，注意命令和之前的不一样

```
cd crawlLianjiaPro
scrapy genspider -t crawl lianjia lianjia.com
```

​	创建爬虫时，我们的命令为`scrapy genspider -t crawl 爬虫名 域名`

​	可以看到，这时候创建出来的深度爬虫和我们原来的不一样，新加了几个功能。

![image-20220614164954214](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220614164954214.png?lastModify=1698732446)



**【知识点一：rules】**

​	CrawlSpider提供了一个非常重要的属性`rules`。

​	`rules`是爬虫规则属性，是包含一个或多个**Rule对象**的列表。每个**Rule对象**对爬取网站的规则都做了定义，CrawlSpider会

读取`rules`的每一个**Rule**并执行对应的爬取逻辑



**【知识点二：Rule对象】**	

​	源代码如下：

   ![image-20220614165656888](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220614165656888.png?lastModify=1698732446)

​	它有7个属性，我们针对其中3个比较重要的参数做介绍，其他的参数可参考官方文档。



- link_extractor：LinkExtractor对象。通过它，Spider可以知道从爬取的页面中根据规则提取链接，并生成Request进行后续爬取。
- callback：回调方法。每次从link_extractor提取链接后生成的Request中对应的回调方法。
- follow：布尔值，表示是否继续跟进爬取，默认为True。



​	我们接下来实际制作一个深度爬虫。

```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule

from crawlLianjiaPro.items import CrawllianjiaproItem

class LianjiaSpider(CrawlSpider):
    name = 'lianjia'
    # allowed_domains = ['lianjia.com']
    start_urls = ['https://bj.lianjia.com/ershoufang/rs/']

    # 实例化一个LinkExtractor对象
    # 链接提取器：根据指定规则(如allow参数)，在页面中进行url的提取
    # 规则allow='正则表达式' 或 allow = ['正则1'，'正则2']： 用于提取正则匹配到的链接

    # # 提取全站链接（特殊）
    # link = LinkExtractor(allow=r"")

    # 提取网站全站的分页链接
    link = LinkExtractor(allow=r'ershoufang/pg\d+')
    # 除此之外，还有规则deny，规则allow_domains

    # 实例化一个Rule对象
    # 规则解析器：接收链接提取器提取到的链接，对其发起请求，然后回调给指定函数
    # 参数link_extractor接收LinkExtractor对象
    # 参数callback接收回调函数的名字
    # 参数follow接收布尔值，表示是否进行跟进爬取
    rule_1 = Rule(link_extractor=link, callback='parse_item', follow=True)

    # 将Rule对象加入到rules中
    rules = (rule_1,)

    # 回调函数
    # 注意点，Rule对象的回调函数尽量不要取名为'parse',比如本例取名叫'parse_item'
    def parse_item(self, response):
        if response:
            # 提示
            print(f'正在爬取{response.url}')
            # 所有的li
            li_ls = response.xpath('//*[@id="content"]/div[1]/ul/li')
            # 遍历
            for li in li_ls:
                # 实例化
                item = CrawllianjiaproItem()
                # 标题
                item['title'] = li.xpath('./div[1]/div[1]/a/text()').extract_first()
                # 传给管道
                yield item
```





**【第四步：定义管道】**

​	pipelines.py

```python
from itemadapter import ItemAdapter


class CrawllianjiaproPipeline:
    def process_item(self, item, spider):
        print(item)
        return item
```





​	**【第五步：修改settings.py】**

1. 禁止robots：`ROBOTSTXT_OBEY = False`

2. 指定日志类型：`LOG_LEVEL = 'ERROR'`  

3. UA伪装：`USER_AGENT = ''`

4. 开启管道：`ITEM_PIPELINES =` 

   

**【第六步：执行爬虫】**

```
scrapy crawl lianjia
```



### 实训十：深度爬虫爬取阳光政务（包含详情页）

​	接下来的实现我只写出关键步骤的代码，不再写完成过程

​	 ![image-20220614172458834](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220614172458834.png?lastModify=1698732446)

```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from sunClawlPro.items import SunItem,SunDetailItem,SunclawlproItem

class SunspiderSpider(CrawlSpider):
    name = 'sunSpider'
    #allowed_domains = ['www.host.com']
    start_urls = ['https://wz.sun0769.com/political/index/politicsNewest?id=1&page=1']



# ## 实现深度爬取方式1(完全使用crawlSpider)： 需要使用两种不同的item()
#     # #实现深度爬取：爬取详情页中的数据
#     # #1.对详情页的url进行捕获
#     # #2.对详情页的url发起请求获取数据
#
#     link = LinkExtractor(allow=r'id=1&page=\d+')  # 提取页码链接
#     detail_link = LinkExtractor(allow=r'index\?id=\d+')
#     rules = (
#         # 解析每一个页码对应页面中的数据
#         Rule(link, callback='parse_item', follow=False),
#         Rule(detail_link,callback='parse_detail_item')
#     )
#
#
#     def parse_item(self, response):
#         li_ls = response.xpath('//ul[@class="title-state-ul"]/li')
#         for li in li_ls:
#             item = SunItem()
#             code = li.xpath('./span[1]/text()').extract_first()  # 编号
#             status = li.xpath('./span[2]/text()').extract_first().strip()  # 状态
#             title = li.xpath('./span[3]/a/text()').extract_first()  # 标题
#             #print(code,status,title)
#             item['code'] = code
#             item['status'] = status
#             item['title'] = title
#             yield item
#
#     def parse_detail_item(self,response):
#         content = response.xpath('//div[@class="details-box"]/pre/text()').extract_first()
#         item = SunDetailItem()
#         item['content'] = content
#         yield item
#         #print(content)
#
# ## 方式1的问题：
#     #1.爬虫文件会向管道中提交两个不同形式的item，管道会接收到两个不同形式的item
#     #2.管道如何区分两种不同形式的item
#         #-在管道中判断接收到的item到底是哪个
#     #3.持久化存储的，目前无法将title和content进行一一匹配



## 方式2（解决了方式1遇到的问题）： 对于分页的链接使用crawlSpider提取并发请求，
#                              对于详情页的链接采用scrapy.Request()手动发起请求

    link = LinkExtractor(allow=r'id=1&page=\d+')  # 提取页码链接
    rules = (
        # 解析每一个页码对应页面中的数据
        Rule(link, callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        li_ls = response.xpath('//ul[@class="title-state-ul"]/li')
        for li in li_ls:
            item = SunclawlproItem()
            code = li.xpath('./span[1]/text()').extract_first()  # 编号
            status = li.xpath('./span[2]/text()').extract_first().strip()  # 状态
            title = li.xpath('./span[3]/a/text()').extract_first()  # 标题
            #print(code,status,title)
            item['code'] = code
            item['status'] = status
            item['title'] = title
            ## 使用scrapy.Request()手动发起请求
            detail_url = 'https://wz.sun0769.com/' + li.xpath('./span[3]/a/@href').extract_first()  # 详情页链接  PS:注意url需要补齐
            yield scrapy.Request(url=detail_url,callback=self.parse_detail_item,meta={'item':item})


    def parse_detail_item(self,response):
        content = response.xpath('//div[@class="details-box"]/pre/text()').extract_first()  #文本
        item = response.meta['item']
        item['content'] = content
        yield item
```

​	 ![image-20220614172533167](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220614172533167.png?lastModify=1698732446)

```python
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class SunclawlproItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    code = scrapy.Field()
    status = scrapy.Field()
    title = scrapy.Field()
    content = scrapy.Field()

class SunItem(scrapy.Item):
    # define the fields for your item here like:
    code = scrapy.Field()
    status = scrapy.Field()
    title = scrapy.Field()


class SunDetailItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    content = scrapy.Field()
```

​	![image-20220614172708667](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220614172708667.png?lastModify=1698732446)

```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter


#方式1的管道
class SunclawlproPipeline1:
    def process_item(self, item, spider):
        if item.__class__.__name__ == 'SunItem':
            title = item['title']
            code = item['code']
            status = item['status']
            print(title,code,status)
        else:
            content = item['content']
            print(content)
        return item


# 方式2的管道
class SunclawlproPipeline2:
    def process_item(self, item, spider):
        print(item)
        return item
```



### 实训十一：Scrapy爬虫Extension（更新中）

​			  （只写关键步骤）





### 实训十二：使用itemloader进行数据清洗（更新中）

​				（只写关键步骤）







## 15.5 Scrapy框架对接Selenium（更新中） 

​	之前我们都是使用Scrapy中的Request对象来发起请求的，其实这个Request发起的请求和request是类似的，均是直接模拟HTTP请求。因此，如果一个网站的内容是经过JS渲染生成的话，那么直接利用Scrapy的Request请求是无法抓取的。

​	在前面的学习中我们也说到了，应对JS渲染的网站有两种方法：一是通过分析Ajax请求接口，二是模拟浏览器进行抓取。

​	第一种方法在Scrapy中的实现和requests模块差不多，也一样面临如果接口有加密就相对难度高的问题；而采用模拟浏览器的办法就可以直接“可见即可爬”。

​	所有，如果我们能够在Scrapy中实现Selenium的对接，就可以实现对JS渲染页面的爬取。同时Scrapy作为一个高性能框架，又可以提供Selenium的效率。

​	

### 实训十三：惠农网爬虫(拦截请求)



​	    `知识点：下载器中间件模拟Selenium请求`



在Scrapy框架中能够对接Selenium的地方有很多，比如说下载器中间件的三个核心方法`process_request()`、`process_response()`、`process_exception()`都可以实现对接。同时不同方法的返回值不同，其产生的效果也不同。

​	其次，我们也可以直接去重写**scrapy.Request**类。

![image-20220614200452012](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220614200452012.png?lastModify=1698732446)

​	可以从架构图中看出，Scheduler会调度Request通过中间件来到Downloader进行HTTP请求。在process_request方法中，当返回Response对象时，会直接转交到process_response(或者Spiders)，不需要再经过Downloader。

​	那也就是说，如果我们实现一个下载器中间件的process_requests改用Selenium请求，最后直接返回一个Response对象给Spider。

#### ① 基本版

 以惠农网采购大厅为例：https://www.cnhnb.com/purchase/

 其他步骤就不写了，我们直接来到中间件



​	middlewares.py

```python
from scrapy import signals
from selenium import webdriver
from scrapy.http import HtmlResponse


class CnhubprojectDownloaderMiddleware:

    # 初始化浏览器
    driver = webdriver.Chrome()

    # 析构时删除浏览器
    def __del__(self):
        self.driver.close()
        self.driver.quit()

    @classmethod
    def from_crawler(cls, crawler):

        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        # 链接信息通过爬虫类获取
        url = request.url
        # 访问链接
        self.driver.get(url)
        # 获取源代码（渲染后）
        html = self.driver.page_source
        # 直接返回响应
        return HtmlResponse(url=request.url, body=html, request=request, encoding='utf-8', status=200)


    def process_response(self, request, response, spider):

        return response

    def process_exception(self, request, exception, spider):

        pass

    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)
```

​	别忘记在settings.py中开启中间件

![image-20220614205034446](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220614205034446.png?lastModify=1698732446)



​	由于通过中间件已经是selenium模拟浏览器请求得到的结果了，我们就在Spider中正常解析即可

```python
import scrapy
import re

from cnhubProject.items import CnhubprojectItem

class CnhubSpider(scrapy.Spider):
    name = 'cnhub'
    # allowed_domains = ['www.cnhnb.com']
    start_urls = ['https://www.cnhnb.com/purchase/']

    # 模板
    base_url = 'https://www.cnhnb.com/purchase/0-0-0-0-0-{}/'
    # 页码
    page = 2

    def parse(self, response):
        # 提示
        print(f"Crawling{response.url}")
        # li_ls
        li_ls = response.xpath('//*[@id="__layout"]/div/div/div[2]/div/div[2]/div[2]/ul/li')
        '//*[@id="__layout"]/div/div/div[2]/div/div[2]/div[2]/ul/li[1]'
        # 遍历
        for li in li_ls:
            # 实例化
            item = CnhubprojectItem()
            # 编号
            code = li.xpath('./div/a/@href').extract_first()
            item['code'] = re.search("\d+", code).group(0)
            # 采购种类
            item['cate_name'] = li.xpath('./div/a/div/div[1]/text()').extract_first()
            # 采购数量
            item['qty'] = li.xpath('./div/a/div/div[2]/text()').extract_first()
            # 期待货物来源
            item['scope_full_name'] = li.xpath('./div/a/div/div[3]/text()').extract_first()
            # 发布人
            item['link_name'] = li.xpath('./div/a/div/div[4]/text()').extract_first()
            # 提交给管道
            yield item

        # 爬取下一页
        if self.page < 11:
            url = self.base_url.format(self.page)
            self.page += 1
            yield scrapy.Request(url=url, callback=self.parse)
```

​		其余步骤略

#### ② 优化版（更新中）

​	在我们基本版的selenium中间件中，实现的功能太粗糙了，简单列举几点：

- Chrome初始化的时候没有指定任何参数，比如headless、proxy等，而且没有把参数可配置化。
- 没有实现异常处理，比如出现TimeException后如何重试
- 加载过程没有设置等待时间（最好是显式和隐式等待，别用强制等待）
- 没有设置Cookie、执行JS或者截图等一系列的拓展功能
- 整个爬取过程编程了阻塞式爬取，同一时刻只有一个页面能被爬取、爬取效率大大降低。









#### ③ 高级优化版

​	https://github.com/Gerapy/GerapySelenium

​	https://github.com/kingron117/scrapy_ajax_utils



### 实训十五：惠农网爬虫--重写scrapy.Request （更新中） 

​	`知识点：重写scrapy.Request`



​	





### 实训十六：网易新闻爬虫（拦截响应）

```
【知识点：下载器中间件伪装响应（结合使用原生request）一起使用】
```

​	一般情况，我们很少会去拦截响应，再去修改响应内容。因为当到`process_response()`的时候，Scrapy已经是发送过一次请求，我们再把Selenium获得的响应伪装过去，其实是非常浪费资源了。

​	但是有一种情况时，站点的一部分是静态的，可以直接用HTTP请求获取；一部分是动态的，要用模拟浏览器访问获取。

​	这时候我们可以为了效率的最大化，Scrapy能请求到的就不用管，请求不到的再改用Selenium。这时候就可以通过拦截响应，先验一遍响应满不满足要求，满足就直接返回给Spider组件做数据解析，不满足要求的重新用Selenium访问获取。



![image-20220615110600364](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220615110600364.png?lastModify=1698732446)	

```python
# -*- coding: utf-8 -*-
import scrapy
from selenium import webdriver
from wangyiPro.items import WangyiproItem
class WangyiSpider(scrapy.Spider):


    name = 'wangyi'
    # allowed_domains = ['www.xxx.com']
    start_urls = ['https://news.163.com/']
    model_urls = [] #每一个板块对应的url

    #实例化了一个全局的浏览器对象
    bro = webdriver.Chrome(executable_path=r'C:\Program Files\Google\Chrome\Application\chromedriver.exe')

    #数据解析：每一个板块对应的url
    def parse(self, response):
        li_list = response.xpath('//*[@id="index2016_wrap"]/div[2]/div[2]/div[2]/div[2]/div/ul/li')
        indexs = [3,4,6,7,8]
        for index in indexs:
            model_li = li_list[index]
            model_url = model_li.xpath('./a/@href').extract_first()
            self.model_urls.append(model_url)
        #对每一个板块的url发起请求
        for url in self.model_urls:
            yield scrapy.Request(url=url,callback=self.parse_model)

    #数据解析：新闻标题+新闻详情页的url（动态加载的数据）
    def parse_model(self,response):
        #直接对response解析新闻标题数据是无法获取该数据（动态加载的数据）
        #response是不满足当下需求的response，需要将其变成满足需求的response
        #满足需求的response就是包含了动态加载数据的response
        #满足需求的response和不满足的response区别在哪里？
            #区别就在于响应数据不同。我们可以使用中间件将不满足需求的响应对象中的响应数据篡改成包含
             # - 了动态加载数据的响应数据，将其变成满足需求的响应对象
        div_list = response.xpath('/html/body/div/div[3]/div[4]/div[1]/div/div/ul/li/div/div')
        for div in div_list:
            title = div.xpath('./div/div[1]/h3/a/text()').extract_first()
            new_detail_url = div.xpath('./div/div[1]/h3/a/@href').extract_first()
            if new_detail_url:
                item = WangyiproItem()
                item['title'] = title
                #对新闻详情页的url发起请求
                yield scrapy.Request(url=new_detail_url,callback=self.parse_new_detail,meta={'item':item})
    def parse_new_detail(self,response):
        #解析新闻内容
        content = response.xpath('//div[@class="post_body"]/p/text()').extract()
        content = ''.join(content)
        item = response.meta['item']
        item['content'] = content

        yield item

    #爬虫类父类的方法，该方法是在爬虫结束最后一刻执行
    def closed(self,spider):
        self.bro.quit()
```



 ![image-20220615110650850](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220615110650850.png?lastModify=1698732446)

```python
# -*- coding: utf-8 -*-

# Define here the models for your spider middleware
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/spider-middleware.html

from scrapy import signals
from time import sleep
from scrapy.http import HtmlResponse#scrapy封装好的响应类

class WangyiproDownloaderMiddleware(object):


    def process_request(self, request, spider):

        return None
    #拦截所有的响应对象
    #整个工程发起的请求：1+5+n，相应也会有1+5+n个响应
    #只有指定的5个响应对象是不满足需求
    #直将不满足需求的5个指定的响应对象的响应数据进行篡改
    def process_response(self, request, response, spider):

        #将拦截到的所有的响应对象中指定的5个响应对象找出
        if request.url in spider.model_urls:
            bro = spider.bro
            #response表示的就是指定的不满足需求的5个响应对象
            #篡改响应数据：首先先获取满足需求的响应数据，将其篡改到响应对象中即可
            #满足需求的响应数据就可以使用selenium获取
            bro.get(request.url) #对五个板块的url发起请求
            sleep(2)
            bro.execute_script('window.scrollTo(0,document.body.scrollHeight)')
            sleep(1)
            #捕获到了板块页面中加载出来的全部数据（包含了动态加载的数据）
            page_text = bro.page_source
            # response.text = page_text
            #返回了一个新的响应对象，新的对象替换原来不满足需求的旧的响应对象
            return HtmlResponse(url=request.url,body=page_text,encoding='utf-8',request=request) # 5
        else:
            return response #1+n

    def process_exception(self, request, exception, spider):

        pass
```



## 15.6 分布式爬虫（更新中）

​	在前面的学习中，我们已经掌握了Scrapy爬虫框架的用法。这些框架都是在同一台主机上运行的，爬取效率比较优先。如果能够用多台主机协同爬取，那么爬取效率必然会成倍增长，这就是分布式爬虫的优势。

​	本节我们就来了解一下分布式爬虫。

### 15.6.1 分布式爬虫理念

#### ① 架构设计

​	Scrapy爬虫虽然是异步加多线程的，但是我们只能在一台主机上运行，所以爬取效率还是有限的。分布式爬虫则是将多台主机组合起来，共同完成一个爬取任务，这将大大提高爬取效率。

​	Scrapy 单机爬虫中有一个本地爬取队列 Queue，这个队列是利用 deque 模块实现的。如果新的 Request 生成就会放到队列里面，随后 Request 被 Scheduler 调度。之后，Request 交给 Downloader 执行爬取，简单的调度架构如下图所示。

​	

![image-20220615113307122](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220615113307122.png?lastModify=1698732446)



​	如果两个 Scheduler 同时从队列里面取 Request，每个 Scheduler 都有其对应的 Downloader，那么在带宽足够、正常爬取且不考虑队列存取压力的情况下，爬取效率会有什么变化？没错，爬取效率会翻倍。

​	这样，Scheduler 可以扩展多个，Downloader 也可以扩展多个。而爬取队列 Queue 必须始终为一个，也就是所谓的共享爬取队列。这样才能保证 Scheduer 从队列里调度某个 Request 之后，其他 Scheduler 不会重复调度此 Request，就可以做到多个 Schduler 同步爬取。这就是分布式爬虫的基本雏形，简单调度架构如图所示。

![image-20220615113347613](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220615113347613.png?lastModify=1698732446)



​	我们需要做的就是在多台主机上同时运行爬虫任务协同爬取，而协同爬取的前提就是共享爬取队列。这样各台主机就不需要各自维护爬取队列，而是从共享爬取队列存取 Request。但是各台主机还是有各自的 Scheduler 和 Downloader，所以调度和下载功能分别完成。如果不考虑队列存取性能消耗，爬取效率还是会成倍提高。



#### ② 维护爬取队列

​	其次，这个爬取队列Queue是需要维护的，考虑到性能问题，最高效的办法是采用 Redis 来维护爬取队列。

​	根据不同的队列情况我们可以选择列表、集合、有序集合等数据结构：

- 列表数据结构有 lpush()、lpop()、rpush()、rpop() 方法，所以我们可以用它来实现一个先进先出式爬取队列，也可以实现一个先进后出栈式爬取队列。

- 集合的元素是无序的且不重复的，这样我们可以非常方便地实现一个随机排序的不重复的爬取队列。

- 有序集合带有分数表示，而 Scrapy 的 Request 也有优先级的控制，所以用有集合我们可以实现一个带优先级调度的队列。

  

#### ③ 去重

​	Scrapy 有自动去重的功能，它的去重使用了 Python 中的集合。这个集合记录了 Scrapy 中每个 Request 的指纹，这个指纹实际上就是 Request 的散列值。

​	我们可以看看 Scrapy 的源代码，如下所示：

```python
import hashlib
def request_fingerprint(request, include_headers=None):
    if include_headers:
        include_headers = tuple(to_bytes(h.lower())
                                 for h in sorted(include_headers))
    cache = _fingerprint_cache.setdefault(request, {})
    if include_headers not in cache:
        fp = hashlib.sha1()
        fp.update(to_bytes(request.method))
        fp.update(to_bytes(canonicalize_url(request.url)))
        fp.update(request.body or b'')
        if include_headers:
            for hdr in include_headers:
                if hdr in request.headers:
                    fp.update(hdr)
                    for v in request.headers.getlist(hdr):
                        fp.update(v)
        cache[include_headers] = fp.hexdigest()
    return cache[include_headers]
```

​	request_fingerprint() 就是计算 Request 指纹的方法，其方法内部使用的是 hashlib 的 sha1() 方法。计算的字段包括 Request 的 Method、URL、Body、Headers 这几部分内容，这里只要有一点不同，那么计算的结果就不同。计算得到的结果是加密后的字符串，也就是指纹。每个 Request 都有独有的指纹，指纹就是一个字符串，判定字符串是否重复比判定 Request 对象是否重复容易得多，所以指纹可以作为判定 Request 是否重复的依据。

​	那么我们如何判定重复呢？Scrapy 是这样实现的，如下所示：

```python
def __init__(self):
    self.fingerprints = set()

def request_seen(self, request):
    fp = self.request_fingerprint(request)
    if fp in self.fingerprints:
        return True
    self.fingerprints.add(fp)
```

​	在去重的类 RFPDupeFilter 中，有一个 request_seen() 方法，这个方法有一个参数 request，它的作用就是检测该 Request 对象是否重复。这个方法调用 request_fingerprint() 获取该 Request 的指纹，检测这个指纹是否存在于 fingerprints 变量中，而 fingerprints 是一个集合，集合的元素都是不重复的。如果指纹存在，那么就返回 True，说明该 Request 是重复的，否则这个指纹加入到集合中。如果下次还有相同的 Request 传递过来，指纹也是相同的，那么这时指纹就已经存在于集合中，Request 对象就会直接判定为重复。这样去重的目的就实现了。

​	Scrapy 的去重过程就是，利用集合元素的不重复特性来实现 Request 的去重。

​	对于分布式爬虫来说，我们肯定不能再用每个爬虫各自的集合来去重了。因为这样还是每个主机单独维护自己的集合，不能做到共享。多台主机如果生成了相同的 Request，只能各自去重，各个主机之间就无法做到去重了。

​	那么要实现去重，这个**指纹集合也需要是共享的**，Redis 正好有集合的存储数据结构，我们可以利用 Redis 的集合作为指纹集合，那么这样去重集合也是利用 Redis 共享的。每台主机新生成 Request 之后，把该 Request 的指纹与集合比对，如果指纹已经存在，说明该 Request 是重复的，否则将 Request 的指纹加入到这个集合中即可。利用同样的原理不同的存储结构我们也实现了分布式 Reqeust 的去重。



#### ④ 防止中断

​	在 Scrapy 中，爬虫运行时的 Request 队列放在内存中。爬虫运行中断后，这个队列的空间就被释放，此队列就被销毁了。所以一旦爬虫运行中断，爬虫再次运行就相当于全新的爬取过程。

​	要做到中断后继续爬取，我们可以将队列中的 Request 保存起来，下次爬取直接读取保存数据即可获取上次爬取的队列。我们在 Scrapy 中指定一个爬取队列的存储路径即可，这个路径使用 JOB_DIR 变量来标识，我们可以用如下命令来实现：

```
scrapy crawl spider -s JOBDIR=crawls/spider
```

​	更加详细的使用方法可以参见官方文档，链接为：https://doc.scrapy.org/en/latest/topics/jobs.html。

​	在 Scrapy 中，我们实际是把爬取队列保存到本地，第二次爬取直接读取并恢复队列即可。那么在分布式架构中我们还用担心这个问题吗？不需要。因为爬取队列本身就是用数据库保存的，如果爬虫中断了，数据库中的 Request 依然是存在的，下次启动就会接着上次中断的地方继续爬取。

​	所以，当 Redis 的队列为空时，爬虫会重新爬取；当 Redis 的队列不为空时，爬虫便会接着上次中断之处继续爬取。



### 15.6.2 Scrapy-redis模块

​	在15.6.1了解完分布式爬虫的理念之后，接下来就需要在程序中实现这个架构了。

​	首先实现一个共享的爬取队列，还要实现去重的功能。另外，重写一个 Scheduer 的实现，使之可以从共享的爬取队列存取 Request。

​	幸运的是，已经有人实现了这些逻辑和架构，并发布成叫 `Scrapy-Redis` 的 Python 包。接下来，我们看看 Scrapy-Redis 的详细工作原理。

​              	![image-20220615120525669](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220615120525669.png?lastModify=1698732446)

#### ① 安装scrapy-redis

```
pip install scrapy-redis
```



#### ② Scrapy-redis的源码实现

​		可参考 https://python3webspider.cuiqingcai.com/14.2scrapyredis-yuan-ma-jie-xi





### 实训十七：Scrapy分布式实现（更新中）

​	在前面我们也说到了，Scrapy-redis已经封装好了大部分的功能，我们只需要导入包并修改系统设置即可。







## 附录一：aiohttp模块

参考链接：https://blog.csdn.net/weixin_38819889/article/details/108632640



​	在15.1.3中，我们已经学习了一下异步爬虫的基本原理和`asyncio`+`aiohttp`的实现。但实际上，在该小节中，我们用`aiohttp`的方法是非常糙的，在附录一中，我们介绍一下`aiohttp`的常见用法。

### A1.1 基本介绍

​	前面介绍的 asyncio 模块内部实现了对 TCP、UDP、SSL 协议的异步操作，但是对于 HTTP 请求的异步操作来说，我们就需要用到 aiohttp 来实现了。

​	aiohttp 是一个基于 asyncio 的异步 HTTP 网络模块，它既提供了服务端，又提供了客户端。其中我们用服务端可以搭建一个支持异步处理的服务器，用于处理请求并返回响应，类似于 Django、Flask、Tornado 等一些 Web 服务器。而客户端我们就可以用来发起请求，就类似于 requests 来发起一个 HTTP 请求然后获得响应，但 requests 发起的是同步的网络请求，而 aiohttp 则发起的是异步的。

​	本小节我们就主要来了解一下 aiohttp 客户端部分的使用。

### A1.2 aiohttp的使用

#### ① 基本案例

```python
import aiohttp
import asyncio


async def fetch(session, url):
    async with session.get(url) as response:
        return await response.text(), response.status


async def main():
    async with aiohttp.ClientSession() as session:
        html, status = await fetch(session, 'http://120.79.0.124:8080/lcvc_ebuy_jsp/shop/index')
        print(f'html: {html[:100]}...')
        print(f'status: {status}')


if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
```

这里网页源码过长，只截取输出了一部分，可以看到我们成功获取了网页的源代码及响应状态码 200，也就完成了一次基本的 HTTP 请求，即我们成功使用 aiohttp 通过异步的方式进行了网页的爬取，当然这个操作用之前我们所讲的 requests 同样也可以做到。

我们可以看到其请求方法的定义和之前有了明显的区别，主要有如下几点：

- 首先在导入库的时候，我们除了必须要引入 aiohttp 这个库之外，还必须要引入 asyncio 这个库，因为要实现异步爬取需要启动协程，而协程则需要借助于 asyncio 里面的事件循环来执行。除了事件循环，asyncio 里面也提供了很多基础的异步操作。
- 异步爬取的方法的定义和之前有所不同，在每个异步方法前面统一要加 async 来修饰。
- with as 语句前面同样需要加 async 来修饰，在 Python 中，with as 语句用于声明一个上下文管理器，能够帮我们自动分配和释放资源，而在异步方法中，with as 前面加上 async 代表声明一个支持异步的上下文管理器。
- 对于一些返回 coroutine 的操作，前面需要加 await 来修饰，如 response 调用 text 方法，查询 API 可以发现其返回的是 coroutine 对象，那么前面就要加 await；而对于状态码来说，其返回值就是一个数值类型，那么前面就不需要加 await。所以，这里可以按照实际情况处理，参考官方文档说明，看看其对应的返回值是怎样的类型，然后决定加不加 await 就可以了。
- 最后，定义完爬取方法之后，实际上是 main 方法调用了 fetch 方法。要运行的话，必须要启用事件循环，事件循环就需要使用 asyncio 库，然后使用 run_until_complete 方法来运行。

#### ② GET请求参数设置

​	对于 URL 参数的设置，我们可以借助于 params 参数，传入一个字典即可，示例如下：

```python
import aiohttp
import asyncio


async def main():
    params = {'name': 'germey', 'age': 25}
    async with aiohttp.ClientSession() as session:
        async with session.get('https://httpbin.org/get', params=params) as response:
            print(await response.text())


if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
```

#### ③ 其他请求类型

​	另外 aiohttp 还支持其他的请求类型，如 POST、PUT、DELETE 等等，这个和 requests 的使用方式有点类似，示例如下：

```python
session.post('http://httpbin.org/post', data=b'data')
session.put('http://httpbin.org/put', data=b'data')
session.delete('http://httpbin.org/delete')
session.head('http://httpbin.org/get')
session.options('http://httpbin.org/get')
session.patch('http://httpbin.org/patch', data=b'data')
```

#### ④ POST 数据

​	对于 POST 表单提交，其对应的请求头的 Content-type 为 `application/x-www-form-urlencoded`，我们可以用如下方式来实现，代码示例如下：

```python
import aiohttp
import asyncio


async def main():
    data = {'name': 'germey', 'age': 25}
    async with aiohttp.ClientSession() as session:
        async with session.post('https://httpbin.org/post', data=data) as response:
            print(await response.text())


if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
```

​	对于 POST JSON 数据提交，其对应的请求头的 Content-type 为 application/json，我们只需要将 post 方法的 data 参数改成 json 即可，代码示例如下：

```python
async def main():
    data = {'name': 'germey', 'age': 25}
    async with aiohttp.ClientSession() as session:
        async with session.post('https://httpbin.org/post', json=data) as response:
            print(await response.text())
```

#### ⑤ 响应字段

​	对于响应来说，我们可以用如下的方法分别获取响应的状态码、响应头、响应体、响应体二进制内容、响应体 JSON 结果，代码示例如下：

```python
async def main():
    data = {'name': 'germey', 'age': 25}
    async with aiohttp.ClientSession() as session:
        async with session.post('https://httpbin.org/post', data=data) as response:
            print('status:', response.status)
            print('headers:', response.headers)
            print('body:', await response.text())
            print('bytes:', await response.read())
            print('json:', await response.json())


if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
```

​	这里我们可以看到有些字段前面需要加 await，有的则不需要。其原则是，如果其返回的是一个 coroutine 对象（如 async 修饰的方法），那么前面就要加 await，具体可以看 aiohttp 的 API，其链接为：https://docs.aiohttp.org/en/stable/client_reference.html。

#### ⑥ 超时设置

​	对于超时的设置，我们可以借助于 ClientTimeout 对象，比如这里我要设置 1 秒的超时，可以这么来实现：

```python
import aiohttp
import asyncio


async def main():
    timeout = aiohttp.ClientTimeout(total=1)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        async with session.get('https://httpbin.org/get') as response:
            print('status:', response.status)


if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
```



#### ⑦ 并发限制

​	由于 aiohttp 可以支持非常大的并发，比如上万、十万、百万都是能做到的，但这么大的并发量，目标网站是很可能在短时间内无法响应的，而且很可能瞬时间将目标网站爬挂掉。所以我们需要控制一下爬取的并发量。

​	在一般情况下，我们可以借助于 asyncio 的 Semaphore 来控制并发量，代码示例如下：

```python
import asyncio
import aiohttp

CONCURRENCY = 5
URL = 'https://www.baidu.com'
semaphore = asyncio.Semaphore(CONCURRENCY)
session = None


async def scrape_api():
    async with semaphore:
        print('scraping', URL)
        async with session.get(URL) as response:
            await asyncio.sleep(1)
            return await response.text()


async def main():
    global session
    session = aiohttp.ClientSession()
    scrape_index_tasks = [asyncio.ensure_future(scrape_api()) for _ in range(10000)]
    await asyncio.gather(*scrape_index_tasks)


if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
```

​	在这里我们声明了 CONCURRENCY 代表爬取的最大并发量为 5，同时声明爬取的目标 URL 为百度。接着我们借助于 Semaphore 创建了一个信号量对象，赋值为 semaphore，这样我们就可以用它来控制最大并发量了。怎么使用呢？我们这里把它直接放置在对应的爬取方法里面，使用 async with 语句将 semaphore 作为上下文对象即可。这样的话，信号量可以控制进入爬取的最大协程数量，最大数量就是我们声明的 CONCURRENCY 的值。

​	在 main 方法里面，我们声明了 10000 个 task，传递给 gather 方法运行。倘若不加以限制，这 10000 个 task 会被同时执行，并发数量太大。但有了信号量的控制之后，同时运行的 task 的数量最大会被控制在 5 个，这样就能给 aiohttp 限制速度了。

​	在这里，aiohttp 的基本使用就介绍这么多，更详细的内容推荐你到官方文档查阅，链接：https://docs.aiohttp.org/







## 附录二：Scrapy框架的正则匹配（更新中）











## 附录三：Spider Middleware组件 （更新中）   









## 附录四：Scrapy框架的常用设置  

​	由于无用设置较多，这里介绍常规设置。了解更多设置直接访问 [Scrapy 2.6.1 设置](https://link.zhihu.com/?target=https%3A//docs.scrapy.org/en/latest/topics/settings.html)



### A4.1 基本设置

**1. 项目名称**

```python
# shell 命令
scrapy startproject Amazon

# 默认的USER_AGENT由它来构成，也作为日志记录的日志名
# 也就是你执行创建项目命令，自动创建，不需要修改。
BOT_NAME = 'Amazon'
```

**2. 应用路径**

```python
# 默认创建不需要修改
SPIDER_MODULES = ['Amazon.spiders']
NEWSPIDER_MODULE = 'Amazon.spiders'
```

**3.User-Agent请求头**

```python
# 默认不需要修改
# USER_AGENT = 'Amazon (+http://www.yourdomain.com)'
```

**4. 爬虫协议**

```python
# 是否遵循爬虫协议，
# 一般网站打开后缀加入robots.txt会有机器人协议的说明，
# 基本无视之，毕竟你遵循了协议就啥也爬不到。
ROBOTSTXT_OBEY = False  # 不遵循协议
```

**5. Cookie操作**

```python
# 是否支持cookie，cookiejar进行操作cookie，默认开启
# COOKIES_ENABLED = False
```

**6. 查看信息记录**

```python
# Telnet用于查看当前爬虫的信息，操作爬虫等...使用telnet ip port ，然后通过命令操作
# TELNETCONSOLE_ENABLED = False
# TELNETCONSOLE_HOST = '127.0.0.1'
# TELNETCONSOLE_PORT = [6023,]
```

**7. 请求头Headers**

```python
# Scrapy发送HTTP请求默认使用的请求头，一般用于重新定向302使用，
# 或者对不同的网站需要进行随机更换请求头的解决简单的反爬。
# 建议重新创建一套随机更换请求头的方式，这里默认不要更改。

# DEFAULT_REQUEST_HEADERS = {
#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#   'Accept-Language': 'en',
#}
```



### A4.2 并发与延迟

**1. 总下载并发设置**

```python
# 下载器总共最大处理的并发请求数,默认值16
# CONCURRENT_REQUESTS = 32
```

**2. 单域名并设置**

```python
# 每个域名能够被执行的最大并发请求数目，默认值8
# CONCURRENT_REQUESTS_PER_DOMAIN = 16
```

**3. 单IP并发设置**

```python
# 能够被单个IP处理的并发请求数，默认值0，代表无限制
# 如果不为零，那CONCURRENT_REQUESTS_PER_DOMAIN会被忽略，即并发数的限制是按照每个IP来计算，而不是每个域名
# 该设置也影响DOWNLOAD_DELAY，如果该值不为零，那么DOWNLOAD_DELAY下载延迟是限制每个IP而不是每个域
# CONCURRENT_REQUESTS_PER_IP = 16
```

**4. 智能限速**

```python
# 对同一网址延迟请求的秒数，如果不设置则固定。
# DOWNLOAD_DELAY = 3
```



### A4.3 智能限速/自动节流

```python
from scrapy.contrib.throttle import AutoThrottle 
#http://scrapy.readthedocs.io/en/latest/topics/autothrottle.html#topics-autothrottle
```

**1.设置目标**

```python
1. 比使用默认的下载延迟对抓取目标站点更好。
2. 自动调整scrapy到最佳的爬取速度，所以用户无需自己调整下载延迟到最佳状态。开发人员只需要定义允许最大并发的请求，其余由该扩展组件自动完成。
```

**2.实现方法**

```python
1. Scrapy下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。
2. 由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在多任务环境下准确测量这些延迟比较困难。这些参数都需要提前设置。
```

**3.限速算法**

```python
1. 自动限速算法基于以下规则调整下载延迟
2. 当收到一个response，对目标站点的下载延迟=收到响应的延迟时间/AUTOTHROTTLE_TARGET_CONCURRENCY
3. 下一次请求的下载延迟就被设置成：对目标站点下载延迟时间和过去的下载延迟时间的平均值
4. 没有达到200个response则不允许降低延迟
5. 下载延迟不能变的比DOWNLOAD_DELAY更低或者比AUTOTHROTTLE_MAX_DELAY更高
```

**4.配置使用**

```python
#开启True，默认False
AUTOTHROTTLE_ENABLED = True
#起始的延迟
AUTOTHROTTLE_START_DELAY = 5
#最小延迟
DOWNLOAD_DELAY = 3
#最大延迟
AUTOTHROTTLE_MAX_DELAY = 10
#每秒并发请求数的平均值，不能高于 CONCURRENT_REQUESTS_PER_DOMAIN或CONCURRENT_REQUESTS_PER_IP，调高了则吞吐量增大强奸目标站点，调低了则对目标站点更加”礼貌“
#每个特定的时间点，scrapy并发请求的数目都可能高于或低于该值，这是爬虫视图达到的建议值而不是硬限制
AUTOTHROTTLE_TARGET_CONCURRENCY = 16.0
#调试
AUTOTHROTTLE_DEBUG = True
CONCURRENT_REQUESTS_PER_DOMAIN = 16
CONCURRENT_REQUESTS_PER_IP = 16
```



### A4.4 爬取深度、方式

**1. 爬虫允许的最大深度**

```python
# 可以通过meta查看当前深度；0表示无深度
# DEPTH_LIMIT = 3
```

**2. 爬取基本原则**

```python
# 爬取时，0表示深度优先Lifo(默认)；1表示广度优先FiFo
# 后进先出，深度优先
# DEPTH_PRIORITY = 0
# SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleLifoDiskQueue'
# SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.LifoMemoryQueue'

# 先进先出，广度优先
# DEPTH_PRIORITY = 1
# SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleFifoDiskQueue'
# SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.FifoMemoryQueue'
```

**3. 调度器队列**

```python
# SCHEDULER = 'scrapy.core.scheduler.Scheduler'
# from scrapy.core.scheduler import Scheduler
```

**4. 访问URL去重**

```python
# DUPEFILTER_CLASS = 'step8_king.duplication.RepeatUrl'
```



### A4.5 中间件、Pipelines、扩展

```python
# 启用或禁用中间件
# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html
# SPIDER_MIDDLEWARES = {
#    'Amazon.middlewares.AmazonSpiderMiddleware': 543,
#}

# 启用或禁用下载器中间件，这里需要使用，否则抓取内容无法使用
# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html
DOWNLOADER_MIDDLEWARES = {
   'Amazon.middlewares.DownMiddleware': 543,
}

DOWNLOADER_MIDDLEWARES_BASE = {
    'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,
    'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,
    'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,
    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400,
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500,
    'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550,
    'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,
    'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,
    'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,
    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,
    'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,
    'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,
    'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,
}

DOWNLOAD_HANDLERS_BASE = {
    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',
    'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',
    'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',
    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',
    'ftp': 'scrapy.core.downloader.handlers.ftp.FTPDownloadHandler',
}

# 启用或禁用扩展
# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html
EXTENSIONS = {
    'scrapy.extensions.corestats.CoreStats': 0,
    'scrapy.extensions.telnet.TelnetConsole': 0,
    'scrapy.extensions.memusage.MemoryUsage': 0,
    'scrapy.extensions.memdebug.MemoryDebugger': 0,
    'scrapy.extensions.closespider.CloseSpider': 0,
    'scrapy.extensions.feedexport.FeedExporter': 0,
    'scrapy.extensions.logstats.LogStats': 0,
    'scrapy.extensions.spiderstate.SpiderState': 0,
    'scrapy.extensions.throttle.AutoThrottle': 0,
}

# 配置项目管道
# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES = {
   # 'Amazon.pipelines.CustomPipeline': 200,
}
```



### A4.6 缓存

```python
# 启用缓存目的用于将已经发送的请求或相应缓存下来，以便以后使用。
from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware
from scrapy.extensions.httpcache import DummyPolicy
from scrapy.extensions.httpcache import FilesystemCacheStorage
# 是否启用缓存策略
# HTTPCACHE_ENABLED = True

# 缓存策略：所有请求均缓存，下次在请求直接访问原来的缓存即可
# HTTPCACHE_POLICY = "scrapy.extensions.httpcache.DummyPolicy"
# 缓存策略：根据Http响应头：Cache-Control、Last-Modified 等进行缓存的策略
# HTTPCACHE_POLICY = "scrapy.extensions.httpcache.RFC2616Policy"

# 缓存超时时间
# HTTPCACHE_EXPIRATION_SECS = 0

# 缓存保存路径
# HTTPCACHE_DIR = 'httpcache'

# 缓存忽略的Http状态码
# HTTPCACHE_IGNORE_HTTP_CODES = []

# 缓存存储的插件
# HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'五  携带cookie的其他思路  
```

​	在实训八中，我们简单的实现了携带cookie进行爬虫的方法，这也就是模拟登陆的思路。但是我们在实训八遇到了一个问题，只获取一次cookie是会失效的，所以实训八的写法其实不太好。

​	我们将在附录五中一起来总结一下携带cookie的写法。

​	不过我这里写的不会向实训中那么详细，我只会展示核心代码，如果有看不懂的可以来问。



## 附录五：Scrapy框架和cookie  

### A5.1 settings中的cookie

#### ① COOKIES_ENABLED 设置

**使用自定义cookie**

```python
# 等于默认注释状态下
COOKIES_ENABLED = True
```

**使用settings的cookie**

```python
COOKIES_ENABLED = False
```

#### ② COOKIES_ENABLED 使用说明

**cookie的添加方式**

在 Scrapy 中 Requests 类，可以使用 cookies 和 headers 两种方式。

**使用自定义的cookie方法**

下面两种设置情况相等

```python
COOKIES_ENABLED = True
# Disable cookies (enabled by default)
# COOKIES_ENABLED = False
```

**COOKIES_ENABLED = True**

1. Scrapy 启动后会自动通过 CookiesMiddleware 中间件，为请求自动添加响应的 cookie。
2. 如果使用 cookies 参数添加 cookie ，会做为额外的 cookie 添加到请求头中，并且在请求头中 cookies 参数的 cookie 数据优先，响应的 response 中的 cookie 不会变。
3. 如果使用 headers 参数添加 cookie，被添加的 cookie 会被响应 cookie 完全覆盖。说明在 headers 里设置的 cookie 无效。



**COOKIES_ENABLED = False**

1. Scrapy 会关闭 CookiesMiddleware 中间件，response 设置的 cookie 也同时失效，无 cookie 的状态。
2. cookies 设置的 cookie 无效，headers 设置的 cookie 有效。



​	**说那么多看个表格就明白了**

| 是否使用cookie | 设置状态                | 说明备注                          |
| -------------- | ----------------------- | --------------------------------- |
| 不使用cookie   | COOKIES_ENABLED = False | headers 里不添加 cookie内容       |
| 自动设置cookie | COOKIES_ENABLED = True  | 用cookies参数全局调整 cookie 设置 |
| 自定义cookie   | COOKIES_ENABLED=False   | 使用 headers 设置 cookie          |

### A5.2 scrapy中的cookie使用

​	参考链接：https://zhuanlan.zhihu.com/p/337212121

#### ① 头信息携带

​	如果是在headers中使用（如实训八）

```python
    def start_requests(self):
        headers = {
            "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36",
            "cookie":"你的微博cookie"
        }
        url = "https://weibo.com/u/{}".format("你的微博id")
        yield Request(url, callback=self.parse, headers=headers)
```

​	那么需要把settings.py的COOKIES_ENABLED设置为false

```python
COOKIES_ENABLED = False
```



#### ② cookies参数

​	如果使用cookies=cookies的方式设置cookie

```python
    # 重载start_requests方法
    def start_requests(self):
        headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:59.0) Gecko/20100101 Firefox/59.0"}
        # 指定cookies
        cookies = {
                    'uuid': '66a0f5e7546b4e068497.1542881406.1.0.0',
                    '_lxsdk_cuid': '1673ae5bfd3c8-0ab24c91d32ccc8-143d7240-144000-1673ae5bfd4c8',
                    '__mta': '222746148.1542881402495.1542881402495.1542881402495.1',
                    'ci': '20',
                    'rvct': '20%2C92%2C282%2C281%2C1',
                    '_lx_utm': 'utm_source%3DBaidu%26utm_medium%3Dorganic',
                    '_lxsdk_s': '1674f401e2a-d02-c7d-438%7C%7C35'}
 
                # 再次请求到详情页，并且声明回调函数callback，dont_filter=True 不进行域名过滤，meta给回调函数传递数据
        yield Request(detailUrl, headers=headers, cookies=cookies, callback=self.detail_parse)
```

​	那么需要把settings.py的COOKIES_ENABLED设置为true

```python
COOKIES_ENABLED = True
```



#### ③ starts_url只有1个

​	如果起始url只有1个可以直接使用

```python
COOKIES_ENABLED = True
```



#### ④ starts_url有多个

​	如果starts_url有多个使用cookiejar

```python
COOKIES_ENABLED = True
```

​	代码示例

```python
class CookieTestSpider(scrapy.Spider):
    name = 'usecookie'
    cookie_dict = {
        "SUB": "你的微博cookie"}
    headers = {
        "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36",
    }
    weibo_id = "你的微博主页id"
    def start_requests(self):
        url = "https://weibo.com/u/{}".format(self.weibo_id)
        yield Request(url, callback=self.parse, headers=self.headers, cookies=self.cookie_dict, meta={'cookiejar': 1})

    def parse(self, response):
        source = response.text
        url = "https://weibo.com/u/{}".format(self.weibo_id)
        if u"我的主页" in source:
            print "找到了"
            yield scrapy.Request(url, headers=self.headers, meta={'cookiejar': response.meta['cookiejar']},
                                 callback=self.parse_page, dont_filter=True)
        else:
            print "没有cookie"

    def parse_page(self, response):
        source = response.text
        if u"我的主页" in source:
            print "又找到了"
        else:
            print "没有cookie"
```





### A5.2 cookie使用思路

#### ① 在spider.py中使用

​	详见实训八，不够完善



#### ② 在middlewares.py中使用

​	先做设置中添加

![image-20220609172035226](file:///C:/Users/ACG1314/AppData/Roaming/Typora/typora-user-images/image-20220609172035226.png?lastModify=1698732446)



​	middleware.py

```python
# Define here the models for your spider middleware
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/spider-middleware.html

from scrapy import signals
import requests
import random

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter


class XueqiuprojectSpiderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the spider middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.

        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        # Called with the results returned from the Spider, after
        # it has processed the response.

        # Must return an iterable of Request, or item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method
        # (from other spider middleware) raises an exception.

        # Should return either None or an iterable of Request or item objects.
        pass

    def process_start_requests(self, start_requests, spider):
        # Called with the start requests of the spider, and works
        # similarly to the process_spider_output() method, except
        # that it doesn’t have a response associated.

        # Must return only requests (not items).
        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)

class XueqiuprojectDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    # 处理 cookie 的函数
    # 将cookie格式化
    def cookie_format(self, cookie):
        '''
        :param cookie_list: 提取到的cookie列表
        :return: 格式化的cookie
        '''
        # print(cookie)
        # 先分隔成列表
        cookie_list = cookie.split(',')
        # 格式化的cookie
        cookie_format = []
        for c in cookie_list:
            # 这里写得糙点了
            one = c.split(';')[0]
            # 判断
            if '=' in one:
                cookie_format.append(one)
        # 拼接
        cookie_format = ';'.join(cookie_format)
        # 返回格式化结果
        return cookie_format

    def process_request(self, request, spider):
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called
        print("process_requests")
        # 用request请求首页，暂时没找到更好的办法
        r = requests.get(url=spider.start_urls[0], headers={"User-Agent": random.choice(spider.USER_AGENT)})
        request.headers['User-Agent'] = random.choice(spider.USER_AGENT)
        request.headers['Cookie'] = self.cookie_format(r.headers['Set-Cookie'])
        # return request
        return None

    def process_response(self, request, response, spider):
        # Called with the response returned from the downloader.

        # Must either;
        # - return a Response object
        # - return a Request object
        # - or raise IgnoreRequest
        # 状态码不等于200时
        # print(response)
        # print(request.headers)
        # print('process_response()')
        if response.status != 200:
            # 状态码不对
            print('状态码不对')
            # print(spider.start_urls[0], spider.USER_AGENT, spider.get_cookie_parse)
            # request._set_url(spider.start_urls[0])
            # request.headers = {"User-Agent": 'random.choice(spider.USER_AGENT)'}
            # request.callback = spider.get_cookie_parse
            # 重新发起请求
            # 疑问：为什么一到return request，爬虫就停止运行（未解决）
            return request

        return response

    def process_exception(self, request, exception, spider):
        # Called when a download handler or a process_request()
        # (from other downloader middleware) raises an exception.

        # Must either:
        # - return None: continue processing this exception
        # - return a Response object: stops process_exception() chain
        # - return a Request object: stops process_exception() chain
        print('process_exception()')
        return None

    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)
```

​	PS：有几处代码写得也比较简单暴力。



## 附录六 未解决问题 

### A6.1 中间件

#### ① 中间件 return request直接结束进程

#### ② 遇到404 301 直接结束爬虫，转交到 close_spider()



## 联动：Scrapy框架爬虫的管理和部署（见第17章）